{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMmphkcxD_iP"
   },
   "source": [
    "# EMG based GR - Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dG534j6kDpiV"
   },
   "source": [
    "# Connecting to Drive, basic defenitions (always run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1654465789713,
     "user": {
      "displayName": "Adi Ben-Ari",
      "userId": "15772198121198769105"
     },
     "user_tz": -180
    },
    "id": "Bvcx8q5Q7t_X",
    "outputId": "a98b7dee-8857-4b46-f649-06d91dac1990"
   },
   "outputs": [],
   "source": [
    "%cd '/content/drive/MyDrive/' # folder path\n",
    "import os\n",
    "path = os.getcwd() \n",
    "print('path: ' + path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KLWvpQxYuYr"
   },
   "source": [
    "**Import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10935,
     "status": "ok",
     "timestamp": 1654465800630,
     "user": {
      "displayName": "Adi Ben-Ari",
      "userId": "15772198121198769105"
     },
     "user_tz": -180
    },
    "id": "WTlgCwEPZOSV",
    "outputId": "137b4050-8341-4a4a-aa18-c84b2c1fc99d"
   },
   "outputs": [],
   "source": [
    "!pip install mne\n",
    "!pip install hmmlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xm6hyCnjYsfb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, lfilter, iirnotch, welch, wiener\n",
    "from math import floor\n",
    "import mne\n",
    "import time\n",
    "import copy\n",
    "from matplotlib.colors import LogNorm\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from hmmlearn import hmm\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmfNHZ61YMk-"
   },
   "source": [
    "**File name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sG8mZhWlYSFz"
   },
   "outputs": [],
   "source": [
    "subject_num = '007'\n",
    "position = '2'\n",
    "session = '2'\n",
    "two_files = False\n",
    "filename = 'data_files/subject_' + subject_num + '/session_' + session + '/GR_' + subject_num + '_pos' + position + '_S' + session + '_Recording_00_SD.edf'\n",
    "\n",
    "filename1 = 'data_files/subject_' + subject_num + '/session_' + session + '/GR_' + subject_num + '_pos' + position + '_S' + session + '_part1_Recording_00_SD_edited.edf'\n",
    "filename2 = 'data_files/subject_' + subject_num + '/session_' + session + '/GR_' + subject_num + '_pos' + position + '_S' + session + '_part2_Recording_00_SD.edf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3MVUqJqcsrR"
   },
   "source": [
    "**Parameters defenitions and helper functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sKpqmpW3cxFx"
   },
   "outputs": [],
   "source": [
    "#sync_annotations = ['Start', 'Release'] # old annotations\n",
    "sync_annotations = ['Start_TwoFingers', 'Release_TwoFingers', 'Start_ThreeFingers', 'Release_ThreeFingers', 'Start_Abduction', 'Release_Abduction', 'Start_Fist', 'Release_Fist', 'Start_Bet', 'Release_Bet', 'Start_Gimel', 'Release_Gimel', 'Start_Het', 'Release_Het', 'Start_Tet', 'Release_Tet', 'Start_Kaf', 'Release_Kaf', 'Start_Nun', 'Release_Nun'] # new annotations\n",
    "sync_annotations_2 = [a + ' ' + str(n) for n in range(1,11) for a in sync_annotations]\n",
    "sync_annotations.extend(sync_annotations_2)\n",
    "\n",
    "window_len = 800\n",
    "gestures_num = 10\n",
    "repetitions = 10\n",
    "\n",
    "# Filter parameters:\n",
    "fl = 20\n",
    "fh = 400\n",
    "fs = 4000\n",
    "order = 4\n",
    "\n",
    "# Decimation:\n",
    "decimation_factor = 1\n",
    "\n",
    "def bandpass_filter(low, high, fs, order):\n",
    "    nyq = 0.5 * fs\n",
    "    low = low / nyq\n",
    "    high = high / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cEQ5aeW7DkDw"
   },
   "source": [
    "# Raw files processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-BUAlT4cUuH"
   },
   "source": [
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12519,
     "status": "ok",
     "timestamp": 1654425989552,
     "user": {
      "displayName": "Adi Ben-Ari",
      "userId": "15772198121198769105"
     },
     "user_tz": -180
    },
    "id": "fKFdF2r_ckYk",
    "outputId": "da06d089-17bc-4603-ff2f-4b3d7910491f"
   },
   "outputs": [],
   "source": [
    "actions = []\n",
    "actions1 = []\n",
    "actions2 = []\n",
    "files_num = 2 if two_files else 1\n",
    "for file_i in range(files_num):\n",
    "  if two_files and file_i==0:\n",
    "    filename = filename1\n",
    "  if two_files and file_i==1:\n",
    "    filename = filename2\n",
    "  # read the raw data and split it into events:\n",
    "  rawEDF = mne.io.read_raw_edf(filename)\n",
    "  rawEDF.load_data()\n",
    "  events = mne.events_from_annotations(rawEDF)\n",
    "  #print(events)\n",
    "  annotations = mne.read_annotations(filename)\n",
    "  event_annotations = annotations.description\n",
    "  events_types = events[0][:,2]\n",
    "  events_indices = events[0][:,0]\n",
    "  if event_annotations.shape[0] != events_indices.shape[0]:\n",
    "      dif = event_annotations.shape[0] - events_indices.shape[0]\n",
    "      event_annotations = event_annotations[dif:]\n",
    "  mask = [a in sync_annotations for a in event_annotations]\n",
    "  events_types = events_types[mask]\n",
    "  events_indices = events_indices[mask]\n",
    "  #relevant_annotations = ['thumb up start','thumb up end','thumb up start','thumb up end','thumb up start','thumb up end','thumb up start','thumb up end','thumb up start','thumb up end','thumb up start','thumb up end','thumb up start','thumb up end','thumb up start','thumb up end','thumb up start','thumb up end','thumb up start','thumb up end','extend23 start','extend23 end','extend23 start','extend23 end','extend23 start','extend23 end','extend23 start','extend23 end','extend23 start','extend23 end','extend23 start','extend23 end','extend23 start','extend23 end','extend23 start','extend23 end','extend23 start','extend23 end','extend23 start','extend23 end','extend123 start','extend123 end','extend123 start','extend123 end','extend123 start','extend123 end','extend123 start','extend123 end','extend123 start','extend123 end','extend123 start','extend123 end','extend123 start','extend123 end','extend123 start','extend123 end','extend123 start','extend123 end','extend123 start','extend123 end','abduction start','abduction end','abduction start','abduction end','abduction start','abduction end','abduction start','abduction end','abduction start','abduction end','abduction start','abduction end','abduction start','abduction end','abduction start','abduction end','abduction start','abduction end','abduction start','abduction end','fist start','fist end','fist start','fist end','fist start','fist end','fist start','fist end','fist start','fist end','fist start','fist end','fist start','fist end','fist start','fist end','fist start','fist end','fist start','fist end','index start','index end','index start','index end','index start','index end','index start','index end','index start','index end','index start','index end','index start','index end','index start','index end','index start','index end','index start','index end']\n",
    "  #relevant_annotations = sync_annotations*(gestures_num*repetitions) # old annotations\n",
    "  relevant_annotations = sync_annotations*(repetitions) # new annotations\n",
    "  print('Number of events:')\n",
    "  print(len(events_indices))\n",
    "  #print('Relevant annotations in files:')\n",
    "  files_annotations = event_annotations[mask]\n",
    "  #print(files_annotations)\n",
    "  print('Wrong annotations in file:')\n",
    "  flag = False\n",
    "  for i in range(len(files_annotations)-1):\n",
    "    if files_annotations[i]==files_annotations[i+1]:\n",
    "      print('annotation number '+str(i)+', at '+str(events_indices[i]/4000)+' seconds')\n",
    "      flag = True\n",
    "  if not flag:\n",
    "    print('None')\n",
    "  \n",
    "  data_pd = rawEDF.to_data_frame(picks='all')\n",
    "  annotations_dict = {}\n",
    "  count = 0\n",
    "  for i in range(len(events_indices)-1):\n",
    "      if 'Start' in files_annotations[i]:\n",
    "          count+=1\n",
    "          event_key = files_annotations[i][6:]\n",
    "          if event_key[-1]>='1' and event_key[-1]<='9':\n",
    "            event_key = event_key[:-2]\n",
    "          if event_key[-1]=='0':\n",
    "            event_key = event_key[:-3]\n",
    "          if event_key not in annotations_dict:\n",
    "            annotations_dict[event_key] = [data_pd.iloc[events_indices[i]:events_indices[i+1],:17]]\n",
    "          else:\n",
    "            annotations_dict[event_key].append(data_pd.iloc[events_indices[i]:events_indices[i+1],:17])\n",
    "          #actions.append(data_pd.iloc[events_indices[i]:events_indices[i+1],:17])\n",
    "  if two_files and file_i==0:\n",
    "    annotations_dict1 = copy.deepcopy(annotations_dict)\n",
    "  if two_files and file_i==1:\n",
    "    annotations_dict2 = copy.deepcopy(annotations_dict)\n",
    "\n",
    "if two_files: # merge actions from the two files\n",
    "  annotations_dict = {}\n",
    "  for key in annotations_dict1.keys():\n",
    "    annotations_dict[key] = annotations_dict1[key]\n",
    "    if key in annotations_dict2.keys():\n",
    "      annotations_dict[key].extend(annotations_dict2[key])\n",
    "\n",
    "actions_order = ['TwoFingers', 'ThreeFingers', 'Abduction', 'Fist', 'Bet', 'Gimel', 'Het', 'Tet', 'Kaf', 'Nun']\n",
    "for a in actions_order:\n",
    "    actions.extend(annotations_dict[a])\n",
    "for k in range(len(actions)):\n",
    "    actions[k] = actions[k].to_numpy()\n",
    "# Finally we have \"actions\" list, whose length is the number of events. Each element is a 16-dimensional time series of the EMG data of a specific realization.\n",
    "events_indices_seg_df = pd.DataFrame(data=events_indices)\n",
    "events_indices_seg_df.to_pickle('segmentation/events_indices_'+subject_num+'_pos'+position+'_session'+session+'.pkl', protocol=4)\n",
    "files_annotations_df = pd.DataFrame(data=files_annotations)\n",
    "files_annotations_df.to_pickle('segmentation/files_annotations_'+subject_num+'_pos'+position+'_session'+session+'.pkl', protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtRPD-tdclTB"
   },
   "source": [
    "Preprocessing - filtering, calculating RMS values (for heat-maps), normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1274,
     "status": "ok",
     "timestamp": 1654264309612,
     "user": {
      "displayName": "Liron Ben-Ari",
      "userId": "04890134261687775011"
     },
     "user_tz": -180
    },
    "id": "0xh-CmGMcyl-",
    "outputId": "7b826955-190d-4dce-c7ae-8b2d615731d9"
   },
   "outputs": [],
   "source": [
    "preprocessed_actions = []\n",
    "preprocessed_actions_rms = []\n",
    "preprocessed_actions_heatmaps = []\n",
    "b, a = bandpass_filter(fl, fh, fs, order)\n",
    "bnotch, anotch = iirnotch(50, 30, fs)\n",
    "bnotch2, anotch2 = iirnotch(100, 30, fs)\n",
    "for ac in range(len(actions)):\n",
    "    realization = np.transpose(actions[ac])\n",
    "    realization = realization[1:,:]\n",
    "    # apply a notch filter and a bandpass filter:\n",
    "    for h in range(realization.shape[0]):\n",
    "        realization[h, :] = lfilter(bnotch, anotch, realization[h, :])\n",
    "        realization[h, :] = lfilter(bnotch2, anotch2, realization[h, :])\n",
    "        realization[h, :] = lfilter(b, a, realization[h, :])\n",
    "\n",
    "    final_array = copy.deepcopy(realization)\n",
    "    final_array = final_array[:,::decimation_factor] # decimation\n",
    "    preprocessed_actions.append(final_array)\n",
    "    preprocessed_actions_rms.append(np.sqrt(np.mean(final_array**2, axis=1)))\n",
    "    max_val = np.max(np.abs(preprocessed_actions_rms[ac]))\n",
    "    min_val = np.min(np.abs(preprocessed_actions_rms[ac]))\n",
    "    preprocessed_actions_rms[ac] = (preprocessed_actions_rms[ac] - min_val) / (max_val - min_val) # normalization\n",
    "    #real_locations = [3, 4, 11, 12, 2, 7, 8, 13, 1, 6, 9, 14, 0, 5, 10, 15] # old setup\n",
    "    real_locations = [9, 10, 7, 8, 16, 13, 4, 1, 15, 12, 5, 2, 14, 11, 6, 3] # new setup\n",
    "    real_locations = [location-1 for location in real_locations] # new setup\n",
    "    preprocessed_actions_heatmaps.append(np.reshape(preprocessed_actions_rms[ac][real_locations],(4,4)))\n",
    "\n",
    "min_window = np.min([int(action.shape[1] / window_len) for action in preprocessed_actions])\n",
    "min_window_index = np.argmin([int(action.shape[1] / window_len) for action in preprocessed_actions])\n",
    "print('Minimum number of windows per realization:')\n",
    "print(str(min_window)+', at realization number '+str(min_window_index))\n",
    "print('Preprocessing completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgJ0Co08RQRA"
   },
   "source": [
    "Visualize the heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 754
    },
    "executionInfo": {
     "elapsed": 6586,
     "status": "ok",
     "timestamp": 1654264316184,
     "user": {
      "displayName": "Liron Ben-Ari",
      "userId": "04890134261687775011"
     },
     "user_tz": -180
    },
    "id": "IO4zQK6rRMH6",
    "outputId": "eaa1d645-05ac-4fb9-d882-66068f857d8c"
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (25,12)\n",
    "\n",
    "fig, axs = plt.subplots(gestures_num,repetitions+1)\n",
    "min = 0.01\n",
    "max = 1\n",
    "for ac in range(len(actions)):\n",
    "    im = axs[int(ac/repetitions), ac%repetitions+1].imshow(preprocessed_actions_heatmaps[ac],cmap='hot')#,norm=LogNorm(vmin=min, vmax=max))\n",
    "for i in range(gestures_num):\n",
    "    for j in range(repetitions+1):\n",
    "        axs[i,j].set_xticks([])\n",
    "        axs[i,j].set_yticks([])\n",
    "gnames = ['Two fingers', 'Three fingers', 'Abduction', 'Fist', 'Bet', 'Gimel', 'Het', 'Tet', 'Kaf', 'Nun']\n",
    "for i in range(gestures_num):\n",
    "    axs[i,0].text(-0.5,0.5,gnames[i], fontsize=18)\n",
    "    axs[i, 0].spines['top'].set_visible(False)\n",
    "    axs[i, 0].spines['right'].set_visible(False)\n",
    "    axs[i, 0].spines['bottom'].set_visible(False)\n",
    "    axs[i, 0].spines['left'].set_visible(False)\n",
    "\n",
    "cbar = fig.colorbar(im, ax=axs.ravel().tolist())\n",
    "cbar.ax.tick_params(labelsize=18) \n",
    "plt.savefig('heatmaps/heatmaps_'+subject_num+'_pos'+position+'_s'+session+'.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVgAQvwDuse7"
   },
   "outputs": [],
   "source": [
    "min_window = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbw2aSeHczjx"
   },
   "source": [
    "Creating data files for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 533,
     "status": "ok",
     "timestamp": 1654264316673,
     "user": {
      "displayName": "Liron Ben-Ari",
      "userId": "04890134261687775011"
     },
     "user_tz": -180
    },
    "id": "4-QYD90Wc71J",
    "outputId": "5c4d0ad3-69dd-4357-df7c-2e56310fda59"
   },
   "outputs": [],
   "source": [
    "\n",
    "value_list = []\n",
    "for ac in range(len(actions)):\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            value_list.append(preprocessed_actions_heatmaps[ac][i][j])\n",
    "\n",
    "output_data = None\n",
    "output_labels = None\n",
    "test_data = None\n",
    "test_labels = None\n",
    "val_data = None\n",
    "val_labels = None\n",
    "actions_dict = {}\n",
    "for act in range(len(preprocessed_actions)):\n",
    "    actions_dict[act] = int(act/10)\n",
    "\n",
    "\n",
    "# Dividing the data to time windows:\n",
    "#print('Number of windows per realization:')\n",
    "#print([int(action.shape[1] / window_len) for action in preprocessed_actions])\n",
    "\n",
    "for ac in range(len(preprocessed_actions)):\n",
    "    preprocessed_actions[ac] = preprocessed_actions[ac][:, :window_len * floor(preprocessed_actions[ac].shape[1] / window_len)]\n",
    "    windows_rms = np.zeros((preprocessed_actions[ac].shape[0], min_window))\n",
    "    for j in range(windows_rms.shape[1]):\n",
    "        mean_square = np.mean(preprocessed_actions[ac][:, window_len * j:window_len * (j + 1)] ** 2, axis=1)\n",
    "        windows_rms[:, j:j + 1] = np.sqrt(np.reshape(mean_square, (mean_square.shape[0], 1)))\n",
    "    preprocessed_actions[ac] = windows_rms\n",
    "    preprocessed_actions[ac] = preprocessed_actions[ac] / np.max(np.abs(preprocessed_actions[ac]))\n",
    "print('The data was divided to time windows!')\n",
    "test_lengths = []\n",
    "for ac in range(len(preprocessed_actions)):\n",
    "    if ac%repetitions == 0:\n",
    "        rand_nums = np.random.randint(0,repetitions,2)\n",
    "        while rand_nums[0]==rand_nums[1]:\n",
    "            rand_nums = np.random.randint(0, repetitions, 2)\n",
    "        index = ac+rand_nums\n",
    "    if ac not in index:\n",
    "        if output_data is None:\n",
    "            output_data = np.transpose(preprocessed_actions[ac])\n",
    "            output_labels = np.transpose(0 * np.ones(preprocessed_actions[ac].shape[1]))\n",
    "        else:\n",
    "            output_data = np.concatenate((output_data, np.transpose(preprocessed_actions[ac])), axis=0)\n",
    "            output_labels = np.concatenate((output_labels, np.transpose(actions_dict[ac] * np.ones(preprocessed_actions[ac].shape[1]))), axis=0)\n",
    "    else:\n",
    "        if test_data is None:\n",
    "            test_data = np.transpose(preprocessed_actions[ac])\n",
    "            test_labels = np.transpose(0 * np.ones(preprocessed_actions[ac].shape[1]))\n",
    "        else:\n",
    "            test_data = np.concatenate((test_data, np.transpose(preprocessed_actions[ac])), axis=0)\n",
    "            test_labels = np.concatenate((test_labels, np.transpose(actions_dict[ac] * np.ones(preprocessed_actions[ac].shape[1]))), axis=0)\n",
    "\n",
    "val_data = copy.deepcopy(output_data[:2*min_window,:])\n",
    "val_labels = copy.deepcopy(output_labels[:2*min_window])\n",
    "for i in range(1,gestures_num):\n",
    "    val_data = np.concatenate((val_data, output_data[(8*i)*min_window:(8*i+2)*min_window,:]))\n",
    "    val_labels = np.concatenate((val_labels, output_labels[(8*i)*min_window:(8*i+2)*min_window]))\n",
    "t = [[j for j in range((8*i+2)*min_window,(8*i+8)*min_window)] for i in range(gestures_num)]\n",
    "flat_list = [item for sublist in t for item in sublist]\n",
    "output_data = output_data[flat_list,:]\n",
    "output_labels = output_labels[flat_list]\n",
    "output_data_df = pd.DataFrame(data=output_data)\n",
    "output_labels_df = pd.DataFrame(data=output_labels)\n",
    "test_data_df = pd.DataFrame(data=test_data)\n",
    "test_labels_df = pd.DataFrame(data=test_labels)\n",
    "lengths_df = pd.DataFrame(data=np.array(test_lengths))\n",
    "val_data_df = pd.DataFrame(data=val_data)\n",
    "val_labels_df = pd.DataFrame(data=val_labels)\n",
    "\n",
    "\n",
    "print('shapes of train data, train labels, validation data, validation labels, test data, test labels:')\n",
    "print(output_data_df.shape)\n",
    "print(output_labels_df.shape)\n",
    "print(val_data.shape)\n",
    "print(val_labels.shape)\n",
    "print(test_data_df.shape)\n",
    "print(test_labels_df.shape)\n",
    "folder_path = 'segmented_files/subject_'+subject_num\n",
    "if not os.path.exists(folder_path):\n",
    "  os.makedirs(folder_path)\n",
    "output_data_df.to_pickle(name_start+'_train_data.pkl', protocol=4)\n",
    "output_labels_df.to_pickle(name_start+'_train_labels.pkl', protocol=4)\n",
    "test_data_df.to_pickle(name_start+'_test_data.pkl', protocol=4)\n",
    "test_labels_df.to_pickle(name_start+'_test_labels.pkl', protocol=4)\n",
    "val_data_df.to_pickle(name_start+'_val_data.pkl', protocol=4)\n",
    "val_labels_df.to_pickle(name_start+'_val_labels.pkl', protocol=4)\n",
    "print('Files saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78U2a-ZLp4dl"
   },
   "source": [
    "Train HMM models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3568,
     "status": "ok",
     "timestamp": 1654264320234,
     "user": {
      "displayName": "Liron Ben-Ari",
      "userId": "04890134261687775011"
     },
     "user_tz": -180
    },
    "id": "Z4OhpRVgp_7w",
    "outputId": "3bdf52c5-bbaf-4030-c2e9-6e192ca2790d"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "#subject_num = '004' ######\n",
    "train_data = None\n",
    "train_labels = None\n",
    "test_data = None\n",
    "test_labels = None\n",
    "name_start = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'+session\n",
    "for filename in [(name_start+'_train_data.pkl', name_start+'_train_labels.pkl', name_start+'_val_data.pkl', name_start+'_val_labels.pkl')]:\n",
    "    data = pd.read_pickle(filename[0])\n",
    "    data = data.to_numpy()\n",
    "    labels = pd.read_pickle(filename[1])\n",
    "    labels = labels.to_numpy()\n",
    "    test_data_part = pd.read_pickle(filename[2])\n",
    "    test_data_part = test_data_part.to_numpy()\n",
    "    test_labels_part = pd.read_pickle(filename[3])\n",
    "    test_labels_part = test_labels_part.to_numpy()\n",
    "\n",
    "    if train_data is None:\n",
    "        train_data = copy.deepcopy(data)\n",
    "        train_labels = copy.deepcopy(labels)\n",
    "        test_data = copy.deepcopy(test_data_part)\n",
    "        test_labels = copy.deepcopy(test_labels_part)\n",
    "    else:\n",
    "        train_data = np.concatenate((train_data, data))\n",
    "        train_labels = np.concatenate((train_labels, labels))\n",
    "        test_data = np.concatenate((test_data, test_data_part))\n",
    "        test_labels = np.concatenate((test_labels, test_labels_part))\n",
    "\n",
    "windows_num = min_window\n",
    "reshaped_train_data = np.zeros((int(train_data.shape[0]/windows_num), windows_num, 16))\n",
    "reshaped_test_data = np.zeros((int(test_data.shape[0]/windows_num), windows_num, 16))\n",
    "for i in range(reshaped_train_data.shape[0]):\n",
    "    reshaped_train_data[i,:,:] = train_data[windows_num*i:windows_num*(i+1)]\n",
    "for i in range(reshaped_test_data.shape[0]):\n",
    "    reshaped_test_data[i,:,:] = test_data[windows_num*i:windows_num*(i+1)]\n",
    "train_data = reshaped_train_data\n",
    "test_data = reshaped_test_data\n",
    "train_labels = train_labels[::windows_num]\n",
    "train_labels = train_labels.flatten()\n",
    "test_labels = test_labels[::windows_num]\n",
    "test_labels = test_labels.flatten()\n",
    "print('Original shapes:')\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n",
    "print('test labels:')\n",
    "print(test_labels.T)\n",
    "print('train labels:')\n",
    "print(train_labels.T)\n",
    "\n",
    "#for comp_num in [4,6,8,10,12]:\n",
    "#    for iterations_num in [1,5,10,50,100]:\n",
    "d = {}\n",
    "for comp_num in [4]:\n",
    "    for iterations_num in [10]:\n",
    "        res = []\n",
    "        tries_num = 1\n",
    "        gestures_num = 10\n",
    "        repetitions = 6\n",
    "        #repetitions = 5\n",
    "        val_num = 2\n",
    "        lengths = [train_data.shape[1] for j in range(repetitions)]\n",
    "        final_res = []\n",
    "        models = []\n",
    "\n",
    "        for i in range(gestures_num):\n",
    "            #X = np.concatenate([train_data[repetitions*i+j,:,:] for j in range(repetitions)])\n",
    "            X = np.concatenate([train_data[repetitions*i+j,:,:] for j in [0,1,2,3,4,5]])\n",
    "\n",
    "            model = hmm.GaussianHMM(n_components=comp_num, covariance_type=\"tied\", n_iter=iterations_num)\n",
    "            model.fit(X, lengths)\n",
    "            models.append(model)\n",
    "            folder_path = 'hmm_models/subject_'+subject_num+'/hmm_'+subject_num+'_pos'+position+'_s'+session \n",
    "            if not os.path.exists(folder_path):\n",
    "                os.makedirs(folder_path)\n",
    "            with open(folder_path+'/hmm_gest'+str(i)+'.pkl', \"wb\") as file: pickle.dump(model, file)\n",
    "        test_res = []\n",
    "        for s in range(gestures_num*val_num):\n",
    "            test_res.append(np.argmax(np.array([models[i].score(test_data[s, :, :]) for i in range(gestures_num)])))\n",
    "\n",
    "        print('test results:')\n",
    "        print(test_res)\n",
    "        print('real labels:')\n",
    "        print(test_labels.T)\n",
    "        acc = np.mean(np.array(test_res)==test_labels)\n",
    "        print('HMM classification accuracy:')\n",
    "        print(np.mean(np.array(test_res)==test_labels))\n",
    "\n",
    "print('total run time: '+str(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6v_q68bwKHX"
   },
   "source": [
    "Generate artificial training data from the traind HMM, and add it to the real training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bjppsx1qxEPy"
   },
   "outputs": [],
   "source": [
    "num_gen = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 461,
     "status": "ok",
     "timestamp": 1654264320669,
     "user": {
      "displayName": "Liron Ben-Ari",
      "userId": "04890134261687775011"
     },
     "user_tz": -180
    },
    "id": "t4gnrceXwRS1",
    "outputId": "7636614b-67db-4536-fead-463a3b05d0dd"
   },
   "outputs": [],
   "source": [
    "train_data = None\n",
    "train_labels = None\n",
    "test_data = None\n",
    "test_labels = None\n",
    "name_start = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'+session\n",
    "for filename in [(name_start+'_train_data.pkl', name_start+'_train_labels.pkl', name_start+'_val_data.pkl', name_start+'_val_labels.pkl')]:\n",
    "    data = pd.read_pickle(filename[0])\n",
    "    data = data.to_numpy()\n",
    "    labels = pd.read_pickle(filename[1])\n",
    "    labels = labels.to_numpy()\n",
    "    test_data_part = pd.read_pickle(filename[2])\n",
    "    test_data_part = test_data_part.to_numpy()\n",
    "    test_labels_part = pd.read_pickle(filename[3])\n",
    "    test_labels_part = test_labels_part.to_numpy()\n",
    "\n",
    "    if train_data is None:\n",
    "        train_data = copy.deepcopy(data)\n",
    "        train_labels = copy.deepcopy(labels)\n",
    "        test_data = copy.deepcopy(test_data_part)\n",
    "        test_labels = copy.deepcopy(test_labels_part)\n",
    "    else:\n",
    "        train_data = np.concatenate((train_data, data))\n",
    "        train_labels = np.concatenate((train_labels, labels))\n",
    "        test_data = np.concatenate((test_data, test_data_part))\n",
    "        test_labels = np.concatenate((test_labels, test_labels_part))\n",
    "\n",
    "\n",
    "res = []\n",
    "comp_num = 4\n",
    "tries_num = 1\n",
    "gestures_num = 10\n",
    "num_windows = min_window\n",
    "final_res = []\n",
    "generated_data = None\n",
    "generated_labels = None\n",
    "for t in range(tries_num):\n",
    "    models = []\n",
    "    for i in range(gestures_num):\n",
    "        folder_path = 'hmm_models/subject_'+subject_num+'/hmm_'+subject_num+'_pos'+position+'_s'+session \n",
    "        file = open(folder_path+'/hmm_gest'+str(i)+'.pkl', \"rb\")\n",
    "        model = pickle.load(file)\n",
    "        for j in range(num_gen):\n",
    "            X, Z = model.sample(num_windows)\n",
    "            if generated_data is None:\n",
    "                generated_data = copy.deepcopy(X)\n",
    "            else:\n",
    "                generated_data = np.concatenate((generated_data,X))\n",
    "        labels = i*np.ones(num_gen*num_windows)\n",
    "        if generated_labels is None:\n",
    "            generated_labels = copy.deepcopy(labels)\n",
    "        else:\n",
    "            generated_labels = np.concatenate((generated_labels, labels))\n",
    "print('generated shapes:')\n",
    "print(generated_data.shape)\n",
    "print(generated_labels.shape)\n",
    "\n",
    "generated_data = np.concatenate((train_data, generated_data))\n",
    "generated_labels = np.concatenate((train_labels.flatten(), generated_labels.flatten()))\n",
    "print('final shapes:')\n",
    "print(generated_data.shape)\n",
    "print(generated_labels.shape)\n",
    "\n",
    "output_data_df = pd.DataFrame(data=generated_data)\n",
    "output_labels_df = pd.DataFrame(data=generated_labels)\n",
    "folder_path = 'hmm_generated_files/subject_'+subject_num\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "name_start_gen = 'hmm_generated_files/subject_'+subject_num+'/gf_'+subject_num+'_pos'+position+'_s'+session\n",
    "output_data_df.to_pickle(name_start_gen+'_train_data.pkl', protocol=4)\n",
    "output_labels_df.to_pickle(name_start_gen+'_train_labels.pkl', protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SswWRqs-ewk0"
   },
   "source": [
    "# Raw files processing with automatic segmentation - new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHiNavLpewk2"
   },
   "source": [
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15594,
     "status": "ok",
     "timestamp": 1654465818893,
     "user": {
      "displayName": "Adi Ben-Ari",
      "userId": "15772198121198769105"
     },
     "user_tz": -180
    },
    "outputId": "c8b21993-f169-41ec-a46a-b51f42a4d16c",
    "id": "7kDEpTBvewk3"
   },
   "outputs": [],
   "source": [
    "actions = []\n",
    "actions1 = []\n",
    "actions2 = []\n",
    "files_num = 2 if two_files else 1\n",
    "for file_i in range(files_num):\n",
    "  if two_files and file_i==0:\n",
    "    filename = filename1\n",
    "  if two_files and file_i==1:\n",
    "    filename = filename2\n",
    "  # read the raw data and split it into events:\n",
    "  rawEDF = mne.io.read_raw_edf(filename)\n",
    "  rawEDF.load_data()\n",
    "  events = mne.events_from_annotations(rawEDF)\n",
    "  #print(events)\n",
    "  annotations = mne.read_annotations(filename)\n",
    "  event_annotations = annotations.description\n",
    "  events_types = events[0][:,2]\n",
    "  events_indices = events[0][:,0]\n",
    "  if event_annotations.shape[0] != events_indices.shape[0]:\n",
    "      dif = event_annotations.shape[0] - events_indices.shape[0]\n",
    "      event_annotations = event_annotations[dif:]\n",
    "  mask = [a in sync_annotations for a in event_annotations]\n",
    "  events_types = events_types[mask]\n",
    "  events_indices = events_indices[mask]\n",
    "  # Read automatic segmentation:\n",
    "  ###########################\n",
    "  seg_filename = 'segmentation/segmentation_final_indices_007_pos2_S2.pkl'\n",
    "  events_indices_auto = pd.read_pickle(seg_filename)\n",
    "  events_indices_auto = events_indices_auto.to_numpy()\n",
    "  for g in range(10):\n",
    "    events_indices[g*20+16:g*20+20] = events_indices_auto[g*20+16:g*20+20,0]\n",
    "  ###########################\n",
    "  #relevant_annotations = ['thumb up start','thumb up end','thumb up start','thumb up end','thumb up start','thumb up end','thumb up start','thumb up end','thumb up start','thumb up end','thumb up start','thumb up end','thumb up start','thumb up end','thumb up start','thumb up end','thumb up start','thumb up end','thumb up start','thumb up end','extend23 start','extend23 end','extend23 start','extend23 end','extend23 start','extend23 end','extend23 start','extend23 end','extend23 start','extend23 end','extend23 start','extend23 end','extend23 start','extend23 end','extend23 start','extend23 end','extend23 start','extend23 end','extend23 start','extend23 end','extend123 start','extend123 end','extend123 start','extend123 end','extend123 start','extend123 end','extend123 start','extend123 end','extend123 start','extend123 end','extend123 start','extend123 end','extend123 start','extend123 end','extend123 start','extend123 end','extend123 start','extend123 end','extend123 start','extend123 end','abduction start','abduction end','abduction start','abduction end','abduction start','abduction end','abduction start','abduction end','abduction start','abduction end','abduction start','abduction end','abduction start','abduction end','abduction start','abduction end','abduction start','abduction end','abduction start','abduction end','fist start','fist end','fist start','fist end','fist start','fist end','fist start','fist end','fist start','fist end','fist start','fist end','fist start','fist end','fist start','fist end','fist start','fist end','fist start','fist end','index start','index end','index start','index end','index start','index end','index start','index end','index start','index end','index start','index end','index start','index end','index start','index end','index start','index end','index start','index end']\n",
    "  #relevant_annotations = sync_annotations*(gestures_num*repetitions) # old annotations\n",
    "  relevant_annotations = sync_annotations*(repetitions) # new annotations\n",
    "  print('Number of events:')\n",
    "  print(len(events_indices))\n",
    "  #print('Relevant annotations in files:')\n",
    "  files_annotations = event_annotations[mask]\n",
    "  #print(files_annotations)\n",
    "  print('Wrong annotations in file:')\n",
    "  flag = False\n",
    "  for i in range(len(files_annotations)-1):\n",
    "    if files_annotations[i]==files_annotations[i+1]:\n",
    "      print('annotation number '+str(i)+', at '+str(events_indices[i]/4000)+' seconds')\n",
    "      flag = True\n",
    "  if not flag:\n",
    "    print('None')\n",
    "  \n",
    "  data_pd = rawEDF.to_data_frame(picks='all')\n",
    "  annotations_dict = {}\n",
    "  count = 0\n",
    "  for i in range(len(events_indices)-1):\n",
    "      if 'Start' in files_annotations[i]:\n",
    "          count+=1\n",
    "          event_key = files_annotations[i][6:]\n",
    "          if event_key[-1]>='1' and event_key[-1]<='9':\n",
    "            event_key = event_key[:-2]\n",
    "          if event_key[-1]=='0':\n",
    "            event_key = event_key[:-3]\n",
    "          if event_key not in annotations_dict:\n",
    "            annotations_dict[event_key] = [data_pd.iloc[events_indices[i]:events_indices[i+1],:17]]\n",
    "          else:\n",
    "            annotations_dict[event_key].append(data_pd.iloc[events_indices[i]:events_indices[i+1],:17])\n",
    "          #actions.append(data_pd.iloc[events_indices[i]:events_indices[i+1],:17])\n",
    "  if two_files and file_i==0:\n",
    "    annotations_dict1 = copy.deepcopy(annotations_dict)\n",
    "  if two_files and file_i==1:\n",
    "    annotations_dict2 = copy.deepcopy(annotations_dict)\n",
    "\n",
    "if two_files: # merge actions from the two files\n",
    "  annotations_dict = {}\n",
    "  for key in annotations_dict1.keys():\n",
    "    annotations_dict[key] = annotations_dict1[key]\n",
    "    if key in annotations_dict2.keys():\n",
    "      annotations_dict[key].extend(annotations_dict2[key])\n",
    "\n",
    "actions_order = ['TwoFingers', 'ThreeFingers', 'Abduction', 'Fist', 'Bet', 'Gimel', 'Het', 'Tet', 'Kaf', 'Nun']\n",
    "for a in actions_order:\n",
    "    actions.extend(annotations_dict[a])\n",
    "for k in range(len(actions)):\n",
    "    actions[k] = actions[k].to_numpy()\n",
    "# Finally we have \"actions\" list, whose length is the number of events. Each element is a 16-dimensional time series of the EMG data of a specific realization.\n",
    "# events_indices_seg_df = pd.DataFrame(data=events_indices)\n",
    "# events_indices_seg_df.to_pickle('segmentation/events_indices_'+subject_num+'_pos'+position+'_session'+session+'.pkl', protocol=4)\n",
    "# files_annotations_df = pd.DataFrame(data=files_annotations)\n",
    "# files_annotations_df.to_pickle('segmentation/files_annotations_'+subject_num+'_pos'+position+'_session'+session+'.pkl', protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIkZSlgiewk6"
   },
   "source": [
    "Preprocessing - filtering, calculating RMS values (for heat-maps), normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2757,
     "status": "ok",
     "timestamp": 1654465821630,
     "user": {
      "displayName": "Adi Ben-Ari",
      "userId": "15772198121198769105"
     },
     "user_tz": -180
    },
    "outputId": "9c4cae93-ef0d-483e-aeb2-fd48b68f37c5",
    "id": "RB0nlPeeewk7"
   },
   "outputs": [],
   "source": [
    "preprocessed_actions = []\n",
    "preprocessed_actions_rms = []\n",
    "preprocessed_actions_heatmaps = []\n",
    "b, a = bandpass_filter(fl, fh, fs, order)\n",
    "bnotch, anotch = iirnotch(50, 30, fs)\n",
    "bnotch2, anotch2 = iirnotch(100, 30, fs)\n",
    "for ac in range(len(actions)):\n",
    "    realization = np.transpose(actions[ac])\n",
    "    realization = realization[1:,:]\n",
    "    # apply a notch filter and a bandpass filter:\n",
    "    for h in range(realization.shape[0]):\n",
    "        realization[h, :] = lfilter(bnotch, anotch, realization[h, :])\n",
    "        realization[h, :] = lfilter(bnotch2, anotch2, realization[h, :])\n",
    "        realization[h, :] = lfilter(b, a, realization[h, :])\n",
    "\n",
    "    final_array = copy.deepcopy(realization)\n",
    "    final_array = final_array[:,::decimation_factor] # decimation\n",
    "    preprocessed_actions.append(final_array)\n",
    "    preprocessed_actions_rms.append(np.sqrt(np.mean(final_array**2, axis=1)))\n",
    "    max_val = np.max(np.abs(preprocessed_actions_rms[ac]))\n",
    "    min_val = np.min(np.abs(preprocessed_actions_rms[ac]))\n",
    "    preprocessed_actions_rms[ac] = (preprocessed_actions_rms[ac] - min_val) / (max_val - min_val) # normalization\n",
    "    #real_locations = [3, 4, 11, 12, 2, 7, 8, 13, 1, 6, 9, 14, 0, 5, 10, 15] # old setup\n",
    "    real_locations = [9, 10, 7, 8, 16, 13, 4, 1, 15, 12, 5, 2, 14, 11, 6, 3] # new setup\n",
    "    real_locations = [location-1 for location in real_locations] # new setup\n",
    "    preprocessed_actions_heatmaps.append(np.reshape(preprocessed_actions_rms[ac][real_locations],(4,4)))\n",
    "\n",
    "min_window = np.min([int(action.shape[1] / window_len) for action in preprocessed_actions])\n",
    "min_window_index = np.argmin([int(action.shape[1] / window_len) for action in preprocessed_actions])\n",
    "print('Minimum number of windows per realization:')\n",
    "print(str(min_window)+', at realization number '+str(min_window_index))\n",
    "print('Preprocessing completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ee0cyqWXewk8"
   },
   "source": [
    "Visualize the heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5C_-kSEewk9"
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (25,12)\n",
    "\n",
    "fig, axs = plt.subplots(gestures_num,repetitions+1)\n",
    "min = 0.01\n",
    "max = 1\n",
    "for ac in range(len(actions)):\n",
    "    im = axs[int(ac/repetitions), ac%repetitions+1].imshow(preprocessed_actions_heatmaps[ac],cmap='hot')#,norm=LogNorm(vmin=min, vmax=max))\n",
    "for i in range(gestures_num):\n",
    "    for j in range(repetitions+1):\n",
    "        axs[i,j].set_xticks([])\n",
    "        axs[i,j].set_yticks([])\n",
    "gnames = ['Two fingers', 'Three fingers', 'Abduction', 'Fist', 'Bet', 'Gimel', 'Het', 'Tet', 'Kaf', 'Nun']\n",
    "for i in range(gestures_num):\n",
    "    axs[i,0].text(-0.5,0.5,gnames[i], fontsize=18)\n",
    "    axs[i, 0].spines['top'].set_visible(False)\n",
    "    axs[i, 0].spines['right'].set_visible(False)\n",
    "    axs[i, 0].spines['bottom'].set_visible(False)\n",
    "    axs[i, 0].spines['left'].set_visible(False)\n",
    "\n",
    "cbar = fig.colorbar(im, ax=axs.ravel().tolist())\n",
    "cbar.ax.tick_params(labelsize=18) \n",
    "plt.savefig('heatmaps/heatmaps_auto_seg_'+subject_num+'_pos'+position+'_s'+session+'.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ktugbdkxewk-"
   },
   "outputs": [],
   "source": [
    "min_window = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDNdoaS3ewk-"
   },
   "source": [
    "Creating data files for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1343,
     "status": "ok",
     "timestamp": 1654465874225,
     "user": {
      "displayName": "Adi Ben-Ari",
      "userId": "15772198121198769105"
     },
     "user_tz": -180
    },
    "outputId": "5652221c-985d-4a75-f32a-5782b41fd693",
    "id": "Wrl5lPXpewk_"
   },
   "outputs": [],
   "source": [
    "\n",
    "value_list = []\n",
    "for ac in range(len(actions)):\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            value_list.append(preprocessed_actions_heatmaps[ac][i][j])\n",
    "\n",
    "output_data = None\n",
    "output_labels = None\n",
    "test_data = None\n",
    "test_labels = None\n",
    "val_data = None\n",
    "val_labels = None\n",
    "actions_dict = {}\n",
    "for act in range(len(preprocessed_actions)):\n",
    "    actions_dict[act] = int(act/10)\n",
    "\n",
    "\n",
    "# Dividing the data to time windows:\n",
    "#print('Number of windows per realization:')\n",
    "#print([int(action.shape[1] / window_len) for action in preprocessed_actions])\n",
    "\n",
    "for ac in range(len(preprocessed_actions)):\n",
    "    preprocessed_actions[ac] = preprocessed_actions[ac][:, :window_len * floor(preprocessed_actions[ac].shape[1] / window_len)]\n",
    "    windows_rms = np.zeros((preprocessed_actions[ac].shape[0], min_window))\n",
    "    for j in range(windows_rms.shape[1]):\n",
    "        mean_square = np.mean(preprocessed_actions[ac][:, window_len * j:window_len * (j + 1)] ** 2, axis=1)\n",
    "        windows_rms[:, j:j + 1] = np.sqrt(np.reshape(mean_square, (mean_square.shape[0], 1)))\n",
    "    preprocessed_actions[ac] = windows_rms\n",
    "    preprocessed_actions[ac] = preprocessed_actions[ac] / np.max(np.abs(preprocessed_actions[ac]))\n",
    "print('The data was divided to time windows!')\n",
    "test_lengths = []\n",
    "for ac in range(len(preprocessed_actions)):\n",
    "    if ac%repetitions == 0:\n",
    "        rand_nums = np.array([8, 9])\n",
    "        # rand_nums = np.random.randint(0,repetitions,2)\n",
    "        # while rand_nums[0]==rand_nums[1]:\n",
    "        #     rand_nums = np.random.randint(0, repetitions, 2)\n",
    "        index = ac+rand_nums\n",
    "    if ac not in index:\n",
    "        if output_data is None:\n",
    "            output_data = np.transpose(preprocessed_actions[ac])\n",
    "            output_labels = np.transpose(0 * np.ones(preprocessed_actions[ac].shape[1]))\n",
    "        else:\n",
    "            output_data = np.concatenate((output_data, np.transpose(preprocessed_actions[ac])), axis=0)\n",
    "            output_labels = np.concatenate((output_labels, np.transpose(actions_dict[ac] * np.ones(preprocessed_actions[ac].shape[1]))), axis=0)\n",
    "    else:\n",
    "        if test_data is None:\n",
    "            test_data = np.transpose(preprocessed_actions[ac])\n",
    "            test_labels = np.transpose(0 * np.ones(preprocessed_actions[ac].shape[1]))\n",
    "        else:\n",
    "            test_data = np.concatenate((test_data, np.transpose(preprocessed_actions[ac])), axis=0)\n",
    "            test_labels = np.concatenate((test_labels, np.transpose(actions_dict[ac] * np.ones(preprocessed_actions[ac].shape[1]))), axis=0)\n",
    "\n",
    "val_data = copy.deepcopy(output_data[:2*min_window,:])\n",
    "val_labels = copy.deepcopy(output_labels[:2*min_window])\n",
    "for i in range(1,gestures_num):\n",
    "    val_data = np.concatenate((val_data, output_data[(8*i)*min_window:(8*i+2)*min_window,:]))\n",
    "    val_labels = np.concatenate((val_labels, output_labels[(8*i)*min_window:(8*i+2)*min_window]))\n",
    "t = [[j for j in range((8*i+2)*min_window,(8*i+8)*min_window)] for i in range(gestures_num)]\n",
    "flat_list = [item for sublist in t for item in sublist]\n",
    "output_data = output_data[flat_list,:]\n",
    "output_labels = output_labels[flat_list]\n",
    "output_data_df = pd.DataFrame(data=output_data)\n",
    "output_labels_df = pd.DataFrame(data=output_labels)\n",
    "# miss_indices = [82]\n",
    "# miss_samples = []\n",
    "# for m in miss_indices:\n",
    "#   m_rep = (m//2)%10\n",
    "#   m_gest = (m//2)//10\n",
    "#   if m_rep == 8 or m_rep == 9:\n",
    "#     miss_samples.append(m_gest*2+m_rep-8)\n",
    "#     #test_data[m_gest*2+m_rep-8] = -1*np.ones(16)\n",
    "#     #test_labels[m_gest*2+m_rep-8] = -1\n",
    "# relevant_samples = [item for item in list(range(20)) if item not in miss_samples]\n",
    "# test_data = test_data[relevant_samples, :]\n",
    "# test_labels = test_labels[relevant_samples]\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n",
    "test_data_df = pd.DataFrame(data=test_data)\n",
    "test_labels_df = pd.DataFrame(data=test_labels)\n",
    "lengths_df = pd.DataFrame(data=np.array(test_lengths))\n",
    "val_data_df = pd.DataFrame(data=val_data)\n",
    "val_labels_df = pd.DataFrame(data=val_labels)\n",
    "\n",
    "\n",
    "print('shapes of train data, train labels, validation data, validation labels, test data, test labels:')\n",
    "print(output_data_df.shape)\n",
    "print(output_labels_df.shape)\n",
    "print(val_data.shape)\n",
    "print(val_labels.shape)\n",
    "print(test_data_df.shape)\n",
    "print(test_labels_df.shape)\n",
    "folder_path = 'segmented_files/subject_'+subject_num\n",
    "if not os.path.exists(folder_path):\n",
    "  os.makedirs(folder_path)\n",
    "###########################\n",
    "name_start = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'+session+'_auto_seg'\n",
    "###########################\n",
    "output_data_df.to_pickle(name_start+'_train_data.pkl', protocol=4)\n",
    "output_labels_df.to_pickle(name_start+'_train_labels.pkl', protocol=4)\n",
    "test_data_df.to_pickle(name_start+'_test_data.pkl', protocol=4)\n",
    "test_labels_df.to_pickle(name_start+'_test_labels.pkl', protocol=4)\n",
    "val_data_df.to_pickle(name_start+'_val_data.pkl', protocol=4)\n",
    "val_labels_df.to_pickle(name_start+'_val_labels.pkl', protocol=4)\n",
    "print('Files saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LkBLrEwewlB"
   },
   "source": [
    "Train HMM models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2716,
     "status": "ok",
     "timestamp": 1654465882441,
     "user": {
      "displayName": "Adi Ben-Ari",
      "userId": "15772198121198769105"
     },
     "user_tz": -180
    },
    "outputId": "53cd4a63-d562-48a8-a34b-4e8c55b87307",
    "id": "1VT_bRAXewlB"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "#subject_num = '004' ######\n",
    "train_data = None\n",
    "train_labels = None\n",
    "test_data = None\n",
    "test_labels = None\n",
    "###########################\n",
    "name_start = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'+session+'_auto_seg'\n",
    "###########################\n",
    "#name_start = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'+session\n",
    "for filename in [(name_start+'_train_data.pkl', name_start+'_train_labels.pkl', name_start+'_val_data.pkl', name_start+'_val_labels.pkl')]:\n",
    "    data = pd.read_pickle(filename[0])\n",
    "    data = data.to_numpy()\n",
    "    labels = pd.read_pickle(filename[1])\n",
    "    labels = labels.to_numpy()\n",
    "    test_data_part = pd.read_pickle(filename[2])\n",
    "    test_data_part = test_data_part.to_numpy()\n",
    "    test_labels_part = pd.read_pickle(filename[3])\n",
    "    test_labels_part = test_labels_part.to_numpy()\n",
    "\n",
    "    if train_data is None:\n",
    "        train_data = copy.deepcopy(data)\n",
    "        train_labels = copy.deepcopy(labels)\n",
    "        test_data = copy.deepcopy(test_data_part)\n",
    "        test_labels = copy.deepcopy(test_labels_part)\n",
    "    else:\n",
    "        train_data = np.concatenate((train_data, data))\n",
    "        train_labels = np.concatenate((train_labels, labels))\n",
    "        test_data = np.concatenate((test_data, test_data_part))\n",
    "        test_labels = np.concatenate((test_labels, test_labels_part))\n",
    "\n",
    "windows_num = min_window\n",
    "reshaped_train_data = np.zeros((int(train_data.shape[0]/windows_num), windows_num, 16))\n",
    "reshaped_test_data = np.zeros((int(test_data.shape[0]/windows_num), windows_num, 16))\n",
    "for i in range(reshaped_train_data.shape[0]):\n",
    "    reshaped_train_data[i,:,:] = train_data[windows_num*i:windows_num*(i+1)]\n",
    "for i in range(reshaped_test_data.shape[0]):\n",
    "    reshaped_test_data[i,:,:] = test_data[windows_num*i:windows_num*(i+1)]\n",
    "train_data = reshaped_train_data\n",
    "test_data = reshaped_test_data\n",
    "train_labels = train_labels[::windows_num]\n",
    "train_labels = train_labels.flatten()\n",
    "test_labels = test_labels[::windows_num]\n",
    "test_labels = test_labels.flatten()\n",
    "print('Original shapes:')\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n",
    "print('test labels:')\n",
    "print(test_labels.T)\n",
    "print('train labels:')\n",
    "print(train_labels.T)\n",
    "\n",
    "#for comp_num in [4,6,8,10,12]:\n",
    "#    for iterations_num in [1,5,10,50,100]:\n",
    "d = {}\n",
    "for comp_num in [4]:\n",
    "    for iterations_num in [10]:\n",
    "        res = []\n",
    "        tries_num = 1\n",
    "        gestures_num = 10\n",
    "        repetitions = 6\n",
    "        #repetitions = 5\n",
    "        val_num = 2\n",
    "        lengths = [train_data.shape[1] for j in range(repetitions)]\n",
    "        final_res = []\n",
    "        models = []\n",
    "\n",
    "        for i in range(gestures_num):\n",
    "            #X = np.concatenate([train_data[repetitions*i+j,:,:] for j in range(repetitions)])\n",
    "            X = np.concatenate([train_data[repetitions*i+j,:,:] for j in [0,1,2,3,4,5]])\n",
    "\n",
    "            model = hmm.GaussianHMM(n_components=comp_num, covariance_type=\"tied\", n_iter=iterations_num)\n",
    "            model.fit(X, lengths)\n",
    "            models.append(model)\n",
    "            folder_path = 'hmm_models/subject_'+subject_num+'/hmm_'+subject_num+'_pos'+position+'_s'+session+'_auto_seg'\n",
    "            if not os.path.exists(folder_path):\n",
    "                os.makedirs(folder_path)\n",
    "            with open(folder_path+'/hmm_gest'+str(i)+'.pkl', \"wb\") as file: pickle.dump(model, file)\n",
    "        test_res = []\n",
    "        for s in range(gestures_num*val_num):\n",
    "            test_res.append(np.argmax(np.array([models[i].score(test_data[s, :, :]) for i in range(gestures_num)])))\n",
    "\n",
    "        print('test results:')\n",
    "        print(test_res)\n",
    "        print('real labels:')\n",
    "        print(test_labels.T)\n",
    "        acc = np.mean(np.array(test_res)==test_labels)\n",
    "        print('HMM classification accuracy:')\n",
    "        print(np.mean(np.array(test_res)==test_labels))\n",
    "\n",
    "print('total run time: '+str(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWZOq2dHewlD"
   },
   "source": [
    "Generate artificial training data from the traind HMM, and add it to the real training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oKEn23bUewlD"
   },
   "outputs": [],
   "source": [
    "num_gen = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 690,
     "status": "ok",
     "timestamp": 1654465891349,
     "user": {
      "displayName": "Adi Ben-Ari",
      "userId": "15772198121198769105"
     },
     "user_tz": -180
    },
    "outputId": "8919c0cd-4017-4a25-8185-f71bfe21f660",
    "id": "bYqspsxaewlD"
   },
   "outputs": [],
   "source": [
    "train_data = None\n",
    "train_labels = None\n",
    "test_data = None\n",
    "test_labels = None\n",
    "name_start = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'+session+'_auto_seg'\n",
    "for filename in [(name_start+'_train_data.pkl', name_start+'_train_labels.pkl', name_start+'_val_data.pkl', name_start+'_val_labels.pkl')]:\n",
    "    data = pd.read_pickle(filename[0])\n",
    "    data = data.to_numpy()\n",
    "    labels = pd.read_pickle(filename[1])\n",
    "    labels = labels.to_numpy()\n",
    "    test_data_part = pd.read_pickle(filename[2])\n",
    "    test_data_part = test_data_part.to_numpy()\n",
    "    test_labels_part = pd.read_pickle(filename[3])\n",
    "    test_labels_part = test_labels_part.to_numpy()\n",
    "\n",
    "    if train_data is None:\n",
    "        train_data = copy.deepcopy(data)\n",
    "        train_labels = copy.deepcopy(labels)\n",
    "        test_data = copy.deepcopy(test_data_part)\n",
    "        test_labels = copy.deepcopy(test_labels_part)\n",
    "    else:\n",
    "        train_data = np.concatenate((train_data, data))\n",
    "        train_labels = np.concatenate((train_labels, labels))\n",
    "        test_data = np.concatenate((test_data, test_data_part))\n",
    "        test_labels = np.concatenate((test_labels, test_labels_part))\n",
    "\n",
    "\n",
    "res = []\n",
    "comp_num = 4\n",
    "tries_num = 1\n",
    "gestures_num = 10\n",
    "num_windows = min_window\n",
    "final_res = []\n",
    "generated_data = None\n",
    "generated_labels = None\n",
    "for t in range(tries_num):\n",
    "    models = []\n",
    "    for i in range(gestures_num):\n",
    "        folder_path = 'hmm_models/subject_'+subject_num+'/hmm_'+subject_num+'_pos'+position+'_s'+session+'_auto_seg'\n",
    "        file = open(folder_path+'/hmm_gest'+str(i)+'.pkl', \"rb\")\n",
    "        model = pickle.load(file)\n",
    "        for j in range(num_gen):\n",
    "            X, Z = model.sample(num_windows)\n",
    "            if generated_data is None:\n",
    "                generated_data = copy.deepcopy(X)\n",
    "            else:\n",
    "                generated_data = np.concatenate((generated_data,X))\n",
    "        labels = i*np.ones(num_gen*num_windows)\n",
    "        if generated_labels is None:\n",
    "            generated_labels = copy.deepcopy(labels)\n",
    "        else:\n",
    "            generated_labels = np.concatenate((generated_labels, labels))\n",
    "print('generated shapes:')\n",
    "print(generated_data.shape)\n",
    "print(generated_labels.shape)\n",
    "\n",
    "generated_data = np.concatenate((train_data, generated_data))\n",
    "generated_labels = np.concatenate((train_labels.flatten(), generated_labels.flatten()))\n",
    "print('final shapes:')\n",
    "print(generated_data.shape)\n",
    "print(generated_labels.shape)\n",
    "\n",
    "output_data_df = pd.DataFrame(data=generated_data)\n",
    "output_labels_df = pd.DataFrame(data=generated_labels)\n",
    "folder_path = 'hmm_generated_files/subject_'+subject_num\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "name_start_gen = 'hmm_generated_files/subject_'+subject_num+'/gf_'+subject_num+'_pos'+position+'_s'+session+'_auto_seg'\n",
    "output_data_df.to_pickle(name_start_gen+'_train_data.pkl', protocol=4)\n",
    "output_labels_df.to_pickle(name_start_gen+'_train_labels.pkl', protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nu_EVkCODudF"
   },
   "source": [
    "# Classification task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNxja_cgdLxF"
   },
   "source": [
    "**Task 1:** 4 classification algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "150u_OLU5uaJ"
   },
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cI-f0Aiawj8b"
   },
   "source": [
    "**Task 1** - Training the CNN: okay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_4WYYbPyYdl"
   },
   "outputs": [],
   "source": [
    "use_hmm = True\n",
    "run = '1'\n",
    "position = '2'\n",
    "train_sessions = ['2']\n",
    "subject_num = '007'\n",
    "pos = -30\n",
    "min_window = 11\n",
    "\n",
    "test_events = 20*len(train_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 60912,
     "status": "ok",
     "timestamp": 1654465976919,
     "user": {
      "displayName": "Adi Ben-Ari",
      "userId": "15772198121198769105"
     },
     "user_tz": -180
    },
    "id": "_3V88pCB6kYj",
    "outputId": "a11cc7a6-20aa-409c-e49b-489b4bf8397e"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Read the files:\n",
    "train_data = None\n",
    "train_labels = None\n",
    "test_data = None\n",
    "test_labels = None\n",
    "if use_hmm:\n",
    "  name_start = 'hmm_generated_files/subject_'+subject_num+'/gf_'+subject_num+'_pos'+position+'_s'\n",
    "  name_start_val = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'\n",
    "else:\n",
    "  name_start = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'\n",
    "  name_start_val = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'\n",
    "for filename in [(name_start+session+'_auto_seg_train_data.pkl', name_start+session+'_auto_seg_train_labels.pkl', name_start_val+session+'_auto_seg_val_data.pkl', name_start_val+session+'_auto_seg_val_labels.pkl') for session in train_sessions]:\n",
    "#for filename in [(pre+'_generated_train_data_10gest.pkl', pre+'_generated_train_labels_10gest.pkl', pre+'_val_data_10gest.pkl', pre+'_val_labels_10gest.pkl') for pre in ['grc1pos1', 'grc1pos2']]:\n",
    "    if filename[0] is not None:\n",
    "        data = pd.read_pickle(filename[0])\n",
    "        data = data.to_numpy()\n",
    "    else:\n",
    "        data = None\n",
    "    print('train data read!')\n",
    "    if filename[1] is not None:\n",
    "        labels = pd.read_pickle(filename[1])\n",
    "        labels = labels.to_numpy()\n",
    "    else:\n",
    "        labels = None\n",
    "    print('train labels read!')\n",
    "    if filename[2] is not None:\n",
    "        test_data_part = pd.read_pickle(filename[2])\n",
    "        test_data_part = test_data_part.to_numpy()\n",
    "    else:\n",
    "        test_data_part = None\n",
    "    print('test data read!')\n",
    "    if filename[3] is not None:\n",
    "        test_labels_part = pd.read_pickle(filename[3])\n",
    "        test_labels_part = test_labels_part.to_numpy()\n",
    "    else:\n",
    "        test_labels_part = None\n",
    "    print('test labels read!')\n",
    "\n",
    "    if train_data is not None and data is not None:\n",
    "        train_data = np.concatenate((train_data, data))\n",
    "        train_labels = np.concatenate((train_labels, labels))\n",
    "    if test_data is not None and test_data_part is not None:\n",
    "        test_data = np.concatenate((test_data, test_data_part))\n",
    "        test_labels = np.concatenate((test_labels, test_labels_part))\n",
    "    if train_data is None or test_data is None:\n",
    "        if data is not None and train_data is None:\n",
    "            train_data = copy.deepcopy(data)\n",
    "            train_labels = copy.deepcopy(labels)\n",
    "        if test_data_part is not None and test_data is None:\n",
    "            test_data = copy.deepcopy(test_data_part)\n",
    "            test_labels = copy.deepcopy(test_labels_part)\n",
    "\n",
    "train_data = np.reshape(train_data, (train_data.shape[0], 4, 4))\n",
    "test_data = np.reshape(test_data, (test_data.shape[0], 4, 4))\n",
    "print('shapes of train data, train labels, test data, test labels:')\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n",
    "per = np.random.permutation(len(train_data))\n",
    "train_data = train_data[per, :]\n",
    "train_labels = train_labels[per]\n",
    "# Data standartization:\n",
    "train_data = train_data - np.mean(train_data, axis=0)\n",
    "train_data = train_data / np.std(train_data, axis=0)\n",
    "test_data = test_data - np.mean(test_data, axis=0)\n",
    "test_data = test_data / np.std(test_data, axis=0)\n",
    "\n",
    "train_data = tf.expand_dims(train_data, axis=-1)\n",
    "test_data = tf.expand_dims(test_data, axis=-1)\n",
    "\n",
    "# Hyper-parameters:\n",
    "if use_hmm:\n",
    "  algo_type='CNN+HMM'\n",
    "else:\n",
    "  algo_type='CNN'\n",
    "reg_strength = 0.0001\n",
    "lr = 5e-4\n",
    "d1 = 0.3\n",
    "epochs_num = 1000 #2000\n",
    "batch_size = 600#1000 #3400\n",
    "#min_window = real_min_window\n",
    "print('min_window='+str(min_window))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(2, 2, padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_strength), name='1'),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.BatchNormalization(name='2'),\n",
    "    tf.keras.layers.Conv2D(4, 2, padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_strength), name='3'),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.BatchNormalization(name='4'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(40, kernel_regularizer=tf.keras.regularizers.l2(reg_strength), name='5'),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.BatchNormalization(name='6'),\n",
    "    tf.keras.layers.Dropout(d1),\n",
    "    tf.keras.layers.Dense(20, kernel_regularizer=tf.keras.regularizers.l2(reg_strength), name='7'),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.BatchNormalization(name='8'),\n",
    "    tf.keras.layers.Dropout(d1),\n",
    "    tf.keras.layers.Dense(10, kernel_regularizer=tf.keras.regularizers.l2(reg_strength), name='new'),\n",
    "    tf.keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "def scheduler(epoch):\n",
    "  if epoch < 600:\n",
    "    return lr\n",
    "  else:\n",
    "    return lr\n",
    "\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#####model.load_weights('weights/CNN/weights_task1_initial_004_run1')\n",
    "history = model.fit(train_data, train_labels, epochs=epochs_num,batch_size=batch_size, callbacks=[callback])\n",
    "model.save_weights('weights/CNN/weights_task1_auto_seg_'+subject_num+'_pos'+position+'_run'+run)\n",
    "#model.load_weights('weights/CNN/weights_task1_'+subject_num+'_pos'+position+'_run'+run)\n",
    "test_loss, test_acc = model.evaluate(test_data,  test_labels)\n",
    "print('\\nSingle frame based validation accuracy:', test_acc)\n",
    "predictions = np.argmax(model.predict(test_data), axis=1)\n",
    "print('Network predictions:')\n",
    "print(predictions)\n",
    "print('Real labels:')\n",
    "print(test_labels.T)\n",
    "\n",
    "# majority voting:\n",
    "predictions_majority = np.zeros(test_events)\n",
    "prev = 0\n",
    "for i in range(test_events):\n",
    "    relevant_pred = predictions[prev:prev+min_window]\n",
    "    predictions_majority[i] = np.argmax(np.bincount(relevant_pred))\n",
    "    prev = prev+int(min_window)\n",
    "print('Network predictions after majority voting:')\n",
    "print(predictions_majority)\n",
    "predictions_majority = predictions_majority.astype(int)\n",
    "real_labels = np.array([0,0,1,1,2,2,3,3,4,4,5,5,6,6,7,7,8,8,9,9]*len(train_sessions))\n",
    "accuracy = np.mean(predictions_majority==real_labels)\n",
    "print('Real labels:')\n",
    "print(real_labels)\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].plot(history.history['accuracy'])\n",
    "axs[0].set(title='Training Accuracy', xlabel='epochs', ylabel='accuracy')\n",
    "axs[1].plot(history.history['loss'])\n",
    "axs[1].set(title='Training Loss', xlabel='epochs', ylabel='loss')\n",
    "#model.summary()\n",
    "plt.show()\n",
    "\n",
    "print('Validation accuracy after majority voting:')\n",
    "print(accuracy)\n",
    "print('Confusion matrix:')\n",
    "plt.imshow(confusion_matrix(real_labels, predictions_majority))\n",
    "plt.colorbar()\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0gwtCEulOz8"
   },
   "source": [
    "**Task 1:** testing the CNN: okay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1654,
     "status": "ok",
     "timestamp": 1654465983424,
     "user": {
      "displayName": "Adi Ben-Ari",
      "userId": "15772198121198769105"
     },
     "user_tz": -180
    },
    "id": "to9sZ0OslS9S",
    "outputId": "a1a3798e-9958-4726-c851-40b1fdc1540f"
   },
   "outputs": [],
   "source": [
    "# Read the files:\n",
    "train_data = None\n",
    "train_labels = None\n",
    "test_data = None\n",
    "test_labels = None\n",
    "name_start = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'\n",
    "for filename in [(name_start+session+'_auto_seg_test_data.pkl', name_start+session+'_auto_seg_test_labels.pkl') for session in train_sessions]:\n",
    "    if filename[0] is not None:\n",
    "        test_data_part = pd.read_pickle(filename[0])\n",
    "        test_data_part = test_data_part.to_numpy()\n",
    "    else:\n",
    "        test_data_part = None\n",
    "    print('test data read!')\n",
    "    if filename[1] is not None:\n",
    "        test_labels_part = pd.read_pickle(filename[1])\n",
    "        test_labels_part = test_labels_part.to_numpy()\n",
    "    else:\n",
    "        test_labels_part = None\n",
    "    print('test labels read!')\n",
    "\n",
    "    if train_data is not None and data is not None:\n",
    "        train_data = np.concatenate((train_data, data))\n",
    "        train_labels = np.concatenate((train_labels, labels))\n",
    "    if test_data is not None and test_data_part is not None:\n",
    "        test_data = np.concatenate((test_data, test_data_part))\n",
    "        test_labels = np.concatenate((test_labels, test_labels_part))\n",
    "    if train_data is None or test_data is None:\n",
    "        if data is not None and train_data is None:\n",
    "            train_data = copy.deepcopy(data)\n",
    "            train_labels = copy.deepcopy(labels)\n",
    "        if test_data_part is not None and test_data is None:\n",
    "            test_data = copy.deepcopy(test_data_part)\n",
    "            test_labels = copy.deepcopy(test_labels_part)\n",
    "\n",
    "test_data = np.reshape(test_data, (test_data.shape[0], 4, 4))\n",
    "print('shapes of test data, test labels:')\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "# Data standartization:\n",
    "test_data = test_data - np.mean(test_data, axis=0)\n",
    "test_data = test_data / np.std(test_data, axis=0)\n",
    "\n",
    "test_data = tf.expand_dims(test_data, axis=-1)\n",
    "\n",
    "model.load_weights('weights/CNN/weights_task1_auto_seg_'+subject_num+'_pos'+position+'_run'+run)\n",
    "test_loss, test_acc = model.evaluate(test_data,  test_labels)\n",
    "print('\\nSingle frame based test accuracy:', test_acc)\n",
    "predictions = np.argmax(model.predict(test_data), axis=1)\n",
    "print('Network predictions:')\n",
    "print(predictions)\n",
    "print('Real labels:')\n",
    "print(test_labels.T)\n",
    "\n",
    "# majority voting:\n",
    "test_events = 20*len(train_sessions)\n",
    "predictions_majority = np.zeros(test_events)\n",
    "prev = 0\n",
    "for i in range(test_events):\n",
    "    relevant_pred = predictions[prev:prev+min_window]\n",
    "    predictions_majority[i] = np.argmax(np.bincount(relevant_pred))\n",
    "    prev = prev+int(min_window)\n",
    "print('Network predictions after majority voting:')\n",
    "print(predictions_majority)\n",
    "predictions_majority = predictions_majority.astype(int)\n",
    "real_labels = np.array([0,0,1,1,2,2,3,3,4,4,5,5,6,6,7,7,8,8,9,9]*len(train_sessions))\n",
    "accuracy = np.mean(predictions_majority==real_labels)\n",
    "print('Real labels:')\n",
    "print(real_labels)\n",
    "\n",
    "print('Test accuracy after majority voting:')\n",
    "print(accuracy)\n",
    "print('Confusion matrix:')\n",
    "mat = confusion_matrix(real_labels, predictions_majority)\n",
    "print(repr(mat))\n",
    "plt.imshow(mat)\n",
    "#plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "folder_path = 'summaries/subject_'+subject_num+'/task1_auto_seg'\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "summary_name = 'summaries/subject_'+subject_num+'/task1_1s/summary_task1_auto_seg_'+subject_num+'_pos'+position+'_type_'+algo_type+'_run'+run\n",
    "f = open(summary_name+'.txt', 'w')\n",
    "f.write('Run summary: task 1, subject number '+subject_num+', position '+position+', algorithm type: '+algo_type+', run '+run+'\\n')\n",
    "f.write('sessions: ')\n",
    "for s_ind, s in enumerate(train_sessions):\n",
    "  if s_ind != len(train_sessions) - 1:\n",
    "    f.write(s+', ')\n",
    "  else:\n",
    "    f.write(s+'\\n')\n",
    "f.write('position: '+position+'\\n')\n",
    "f.write('regularization strength: '+str(reg_strength)+'\\n')\n",
    "f.write('learning rate: '+str(lr)+'\\n')\n",
    "f.write('dropout: '+str(d1)+'\\n')\n",
    "f.write('number of epochs: '+str(epochs_num)+'\\n')\n",
    "f.write('batch size: '+str(batch_size)+'\\n')\n",
    "f.write('minimal window length (in samples): '+str(min_window)+'\\n')\n",
    "f.write('Single frame based validation accuracy: '+str(test_acc)+'\\n')\n",
    "f.write('Test accuracy after majority voting: '+str(accuracy)+'\\n')\n",
    "f.write('weights file: weights_task1_'+subject_num+'_pos'+position+'_run'+run+'\\n')\n",
    "f.close()\n",
    "\n",
    "confusion_matrix_df = pd.DataFrame(data=mat)\n",
    "confusion_matrix_df.to_pickle(folder_path+'/CNN_mat_run'+run+'.pkl', protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOIeuGdb505g"
   },
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_XC0b-Gws8q"
   },
   "source": [
    "Training the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3F7VJGfwxL_"
   },
   "outputs": [],
   "source": [
    "use_hmm = True\n",
    "run = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 83022,
     "status": "ok",
     "timestamp": 1654334005649,
     "user": {
      "displayName": "Adi Ben-Ari",
      "userId": "15772198121198769105"
     },
     "user_tz": -180
    },
    "id": "vi5OWTEjwzFz",
    "outputId": "561ee9db-503c-4c17-fa8d-c06cac492179"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Read the files:\n",
    "train_data = None\n",
    "train_labels = None\n",
    "test_data = None\n",
    "test_labels = None\n",
    "if use_hmm:\n",
    "  name_start = 'hmm_generated_files/subject_'+subject_num+'/gf_'+subject_num+'_pos'+position+'_s'\n",
    "  name_start_val = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'\n",
    "else:\n",
    "  name_start = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'\n",
    "  name_start_val = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'\n",
    "for filename in [(name_start+session+'_train_data.pkl', name_start+session+'_train_labels.pkl', name_start_val+session+'_val_data.pkl', name_start_val+session+'_val_labels.pkl') for session in train_sessions]:\n",
    "#for filename in [(pre+'_generated_train_data_10gest.pkl', pre+'_generated_train_labels_10gest.pkl', pre+'_val_data_10gest.pkl', pre+'_val_labels_10gest.pkl') for pre in ['grc1pos1', 'grc1pos2']]:\n",
    "    if filename[0] is not None:\n",
    "        data = pd.read_pickle(filename[0])\n",
    "        data = data.to_numpy()\n",
    "    else:\n",
    "        data = None\n",
    "    print('train data read!')\n",
    "    if filename[1] is not None:\n",
    "        labels = pd.read_pickle(filename[1])\n",
    "        labels = labels.to_numpy()\n",
    "    else:\n",
    "        labels = None\n",
    "    print('train labels read!')\n",
    "    if filename[2] is not None:\n",
    "        test_data_part = pd.read_pickle(filename[2])\n",
    "        test_data_part = test_data_part.to_numpy()\n",
    "    else:\n",
    "        test_data_part = None\n",
    "    print('test data read!')\n",
    "    if filename[3] is not None:\n",
    "        test_labels_part = pd.read_pickle(filename[3])\n",
    "        test_labels_part = test_labels_part.to_numpy()\n",
    "    else:\n",
    "        test_labels_part = None\n",
    "    print('test labels read!')\n",
    "\n",
    "    if train_data is not None and data is not None:\n",
    "        train_data = np.concatenate((train_data, data))\n",
    "        train_labels = np.concatenate((train_labels, labels))\n",
    "    if test_data is not None and test_data_part is not None:\n",
    "        test_data = np.concatenate((test_data, test_data_part))\n",
    "        test_labels = np.concatenate((test_labels, test_labels_part))\n",
    "    if train_data is None or test_data is None:\n",
    "        if data is not None and train_data is None:\n",
    "            train_data = copy.deepcopy(data)\n",
    "            train_labels = copy.deepcopy(labels)\n",
    "        if test_data_part is not None and test_data is None:\n",
    "            test_data = copy.deepcopy(test_data_part)\n",
    "            test_labels = copy.deepcopy(test_labels_part)\n",
    "\n",
    "\n",
    "# Hyper-parameters:\n",
    "if use_hmm:\n",
    "  algo_type='RNN+HMM'\n",
    "else:\n",
    "  algo_type='RNN'\n",
    "reg_strength = 0.0001\n",
    "lr = 5e-3\n",
    "d1 = 0\n",
    "#min_angle = 1/18\n",
    "#max_shift_h = 0.05\n",
    "#max_shift_w = 0.05\n",
    "epochs_num = 1500 #1500\n",
    "batch_size = 1000 # full batch\n",
    "print('min_window='+str(min_window))\n",
    "\n",
    "\n",
    "windows_num = min_window\n",
    "reshaped_train_data = np.zeros((int(train_data.shape[0]/windows_num), windows_num, 16))\n",
    "reshaped_test_data = np.zeros((int(test_data.shape[0]/windows_num), windows_num, 16))\n",
    "for i in range(reshaped_train_data.shape[0]):\n",
    "    reshaped_train_data[i,:,:] = train_data[windows_num*i:windows_num*(i+1)]\n",
    "for i in range(reshaped_test_data.shape[0]):\n",
    "    reshaped_test_data[i,:,:] = test_data[windows_num*i:windows_num*(i+1)]\n",
    "train_data = reshaped_train_data\n",
    "test_data = reshaped_test_data\n",
    "train_labels = train_labels[::windows_num]\n",
    "test_labels = test_labels[::windows_num]\n",
    "print('shapes:')\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "\n",
    "per = np.random.permutation(len(train_data))\n",
    "train_data = train_data[per, :]\n",
    "train_labels = train_labels[per]\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(12, dropout=d1, input_shape=(windows_num,16), name='lstm2'),\n",
    "    tf.keras.layers.Dense(10, kernel_regularizer=tf.keras.regularizers.l2(reg_strength), name='new'),\n",
    "    tf.keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "def scheduler(epoch):\n",
    "  if epoch < 600:\n",
    "    return lr\n",
    "  else:\n",
    "    return lr\n",
    "\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_data, train_labels, epochs=epochs_num,batch_size=batch_size, callbacks=[callback])\n",
    "model.save_weights('weights/RNN/weights_task1_1s_'+subject_num+'_pos'+position+'_run'+run)\n",
    "#model.load_weights('weights/RNN/weights_task1_'+subject_num+'_pos'+position+'_run'+run)\n",
    "test_loss, test_acc = model.evaluate(test_data,  test_labels)#, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "\n",
    "predictions = np.argmax(model.predict(test_data), axis=1)\n",
    "cmat = confusion_matrix(test_labels, predictions)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].plot(history.history['accuracy'])\n",
    "axs[0].set(title='Training Accuracy', xlabel='epochs', ylabel='accuracy')\n",
    "axs[1].plot(history.history['loss'])\n",
    "axs[1].set(title='Training Loss', xlabel='epochs', ylabel='loss')\n",
    "#model.summary()\n",
    "plt.figure()\n",
    "\n",
    "print('Confusion matrix:')\n",
    "plt.imshow(cmat)\n",
    "plt.colorbar()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ck4728DMF3Sl"
   },
   "source": [
    "Testing the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3247,
     "status": "ok",
     "timestamp": 1654334013717,
     "user": {
      "displayName": "Adi Ben-Ari",
      "userId": "15772198121198769105"
     },
     "user_tz": -180
    },
    "id": "Mxr82MDKF483",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b5363732-b7fc-4e7c-8a50-fe994fc6ea7f"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Read the files:\n",
    "train_data = None\n",
    "train_labels = None\n",
    "test_data = None\n",
    "test_labels = None\n",
    "if use_hmm:\n",
    "  name_start = 'hmm_generated_files/subject_'+subject_num+'/gf_'+subject_num+'_pos'+position+'_s'\n",
    "  name_start_val = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'\n",
    "else:\n",
    "  name_start = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'\n",
    "  name_start_val = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'\n",
    "for filename in [(name_start+session+'_train_data.pkl', name_start+session+'_train_labels.pkl', name_start_val+session+'_test_data.pkl', name_start_val+session+'_test_labels.pkl') for session in train_sessions]:\n",
    "#for filename in [(pre+'_generated_train_data_10gest.pkl', pre+'_generated_train_labels_10gest.pkl', pre+'_val_data_10gest.pkl', pre+'_val_labels_10gest.pkl') for pre in ['grc1pos1', 'grc1pos2']]:\n",
    "    if filename[0] is not None:\n",
    "        data = pd.read_pickle(filename[0])\n",
    "        data = data.to_numpy()\n",
    "    else:\n",
    "        data = None\n",
    "    print('train data read!')\n",
    "    if filename[1] is not None:\n",
    "        labels = pd.read_pickle(filename[1])\n",
    "        labels = labels.to_numpy()\n",
    "    else:\n",
    "        labels = None\n",
    "    print('train labels read!')\n",
    "    if filename[2] is not None:\n",
    "        test_data_part = pd.read_pickle(filename[2])\n",
    "        test_data_part = test_data_part.to_numpy()\n",
    "    else:\n",
    "        test_data_part = None\n",
    "    print('test data read!')\n",
    "    if filename[3] is not None:\n",
    "        test_labels_part = pd.read_pickle(filename[3])\n",
    "        test_labels_part = test_labels_part.to_numpy()\n",
    "    else:\n",
    "        test_labels_part = None\n",
    "    print('test labels read!')\n",
    "\n",
    "    if train_data is not None and data is not None:\n",
    "        train_data = np.concatenate((train_data, data))\n",
    "        train_labels = np.concatenate((train_labels, labels))\n",
    "    if test_data is not None and test_data_part is not None:\n",
    "        test_data = np.concatenate((test_data, test_data_part))\n",
    "        test_labels = np.concatenate((test_labels, test_labels_part))\n",
    "    if train_data is None or test_data is None:\n",
    "        if data is not None and train_data is None:\n",
    "            train_data = copy.deepcopy(data)\n",
    "            train_labels = copy.deepcopy(labels)\n",
    "        if test_data_part is not None and test_data is None:\n",
    "            test_data = copy.deepcopy(test_data_part)\n",
    "            test_labels = copy.deepcopy(test_labels_part)\n",
    "\n",
    "\n",
    "# Hyper-parameters:\n",
    "if use_hmm:\n",
    "  algo_type='RNN+HMM'\n",
    "else:\n",
    "  algo_type='RNN'\n",
    "reg_strength = 0.0001\n",
    "lr = 1e-2\n",
    "d1 = 0.1\n",
    "#min_angle = 1/18\n",
    "#max_shift_h = 0.05\n",
    "#max_shift_w = 0.05\n",
    "epochs_num = 2000 #1500\n",
    "batch_size = 500\n",
    "print('min_window='+str(min_window))\n",
    "\n",
    "\n",
    "windows_num = min_window\n",
    "reshaped_train_data = np.zeros((int(train_data.shape[0]/windows_num), windows_num, 16))\n",
    "reshaped_test_data = np.zeros((int(test_data.shape[0]/windows_num), windows_num, 16))\n",
    "for i in range(reshaped_train_data.shape[0]):\n",
    "    reshaped_train_data[i,:,:] = train_data[windows_num*i:windows_num*(i+1)]\n",
    "for i in range(reshaped_test_data.shape[0]):\n",
    "    reshaped_test_data[i,:,:] = test_data[windows_num*i:windows_num*(i+1)]\n",
    "train_data = reshaped_train_data\n",
    "test_data = reshaped_test_data\n",
    "train_labels = train_labels[::windows_num]\n",
    "test_labels = test_labels[::windows_num]\n",
    "print('shapes:')\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "\n",
    "per = np.random.permutation(len(train_data))\n",
    "train_data = train_data[per, :]\n",
    "train_labels = train_labels[per]\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(12, dropout=d1, input_shape=(windows_num,16), name='lstm2'),\n",
    "    tf.keras.layers.Dense(10, kernel_regularizer=tf.keras.regularizers.l2(reg_strength), name='new'),\n",
    "    tf.keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "def scheduler(epoch):\n",
    "  if epoch < 600:\n",
    "    return lr\n",
    "  else:\n",
    "    return lr\n",
    "\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#history = model.fit(train_data, train_labels, epochs=epochs_num,batch_size=batch_size, callbacks=[callback])\n",
    "#model.save_weights('weights/RNN/weights_task1_'+subject_num+'_pos'+position+'_run'+run)\n",
    "model.load_weights('weights/RNN/weights_task1_1s_'+subject_num+'_pos'+position+'_run'+run)\n",
    "test_loss, test_acc = model.evaluate(test_data,  test_labels)#, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "\n",
    "predictions = np.argmax(model.predict(test_data), axis=1)\n",
    "cmat = confusion_matrix(test_labels, predictions)\n",
    "\n",
    "folder_path = 'summaries/subject_'+subject_num+'/task1_1s'\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "summary_name = 'summaries/subject_'+subject_num+'/task1_1s/summary_task1_1s_'+subject_num+'_pos'+position+'_type_'+algo_type+'_run'+run\n",
    "f = open(summary_name+'.txt', 'w')\n",
    "f.write('Run summary: task 1, subject number '+subject_num+', position '+position+', algorithm type: '+algo_type+', run '+run+'\\n')\n",
    "f.write('sessions: ')\n",
    "for s_ind, s in enumerate(train_sessions):\n",
    "  if s_ind != len(train_sessions) - 1:\n",
    "    f.write(s+', ')\n",
    "  else:\n",
    "    f.write(s+'\\n')\n",
    "f.write('position: '+position+'\\n')\n",
    "f.write('regularization strength: '+str(reg_strength)+'\\n')\n",
    "f.write('learning rate: '+str(lr)+'\\n')\n",
    "f.write('dropout: '+str(d1)+'\\n')\n",
    "f.write('number of epochs: '+str(epochs_num)+'\\n')\n",
    "f.write('batch size: '+str(batch_size)+'\\n')\n",
    "f.write('minimal window length (in samples): '+str(min_window)+'\\n')\n",
    "f.write('Test accuracy after majority voting: '+str(test_acc)+'\\n')\n",
    "f.write('weights file: weights_task1_'+subject_num+'_pos'+position+'_run'+run+'\\n')\n",
    "f.close()\n",
    "\n",
    "confusion_matrix_df = pd.DataFrame(data=mat)\n",
    "confusion_matrix_df.to_pickle(folder_path+'/RNN_mat.pkl', protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-drHP9U56QS"
   },
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--SCYWaf8hEy"
   },
   "source": [
    "**Task 1** - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Dx_anVl-HwW"
   },
   "outputs": [],
   "source": [
    "use_hmm = False\n",
    "run = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6299,
     "status": "ok",
     "timestamp": 1654334032791,
     "user": {
      "displayName": "Adi Ben-Ari",
      "userId": "15772198121198769105"
     },
     "user_tz": -180
    },
    "id": "oBFfZjNe8geE",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "outputId": "01ff2b8c-4ffc-47de-aad2-7a64cc7994a5"
   },
   "outputs": [],
   "source": [
    "k_KNN = 1\n",
    "\n",
    "# Read the files:\n",
    "train_data = None\n",
    "train_labels = None\n",
    "test_data = None\n",
    "test_labels = None\n",
    "if use_hmm:\n",
    "  name_start = 'hmm_generated_files/subject_'+subject_num+'/gf_'+subject_num+'_pos'+position+'_s'\n",
    "  name_start_val = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'\n",
    "else:\n",
    "  name_start = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'\n",
    "  name_start_val = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'\n",
    "for filename in [(name_start+session+'_train_data.pkl', name_start+session+'_train_labels.pkl', name_start_val+session+'_test_data.pkl', name_start_val+session+'_test_labels.pkl') for session in train_sessions]:\n",
    "#for filename in [(pre+'_generated_train_data_10gest.pkl', pre+'_generated_train_labels_10gest.pkl', pre+'_val_data_10gest.pkl', pre+'_val_labels_10gest.pkl') for pre in ['grc1pos1', 'grc1pos2']]:\n",
    "    if filename[0] is not None:\n",
    "        data = pd.read_pickle(filename[0])\n",
    "        data = data.to_numpy()\n",
    "    else:\n",
    "        data = None\n",
    "    print('train data read!')\n",
    "    if filename[1] is not None:\n",
    "        labels = pd.read_pickle(filename[1])\n",
    "        labels = labels.to_numpy()\n",
    "    else:\n",
    "        labels = None\n",
    "    print('train labels read!')\n",
    "    if filename[2] is not None:\n",
    "        test_data_part = pd.read_pickle(filename[2])\n",
    "        test_data_part = test_data_part.to_numpy()\n",
    "    else:\n",
    "        test_data_part = None\n",
    "    print('test data read!')\n",
    "    if filename[3] is not None:\n",
    "        test_labels_part = pd.read_pickle(filename[3])\n",
    "        test_labels_part = test_labels_part.to_numpy()\n",
    "    else:\n",
    "        test_labels_part = None\n",
    "    print('test labels read!')\n",
    "\n",
    "    if train_data is not None and data is not None:\n",
    "        train_data = np.concatenate((train_data, data))\n",
    "        train_labels = np.concatenate((train_labels, labels))\n",
    "    if test_data is not None and test_data_part is not None:\n",
    "        test_data = np.concatenate((test_data, test_data_part))\n",
    "        test_labels = np.concatenate((test_labels, test_labels_part))\n",
    "    if train_data is None or test_data is None:\n",
    "        if data is not None and train_data is None:\n",
    "            train_data = copy.deepcopy(data)\n",
    "            train_labels = copy.deepcopy(labels)\n",
    "        if test_data_part is not None and test_data is None:\n",
    "            test_data = copy.deepcopy(test_data_part)\n",
    "            test_labels = copy.deepcopy(test_labels_part)\n",
    "\n",
    "KNN = KNeighborsClassifier(n_neighbors=k_KNN)\n",
    "KNN.fit(train_data, train_labels)\n",
    "predictions = KNN.predict(test_data).astype('int')\n",
    "single_acc = np.mean(predictions==test_labels.T)\n",
    "print('KNN single frame accuracy: '+str(single_acc))\n",
    "\n",
    "# majority voting:\n",
    "predictions_majority = np.zeros(test_events)\n",
    "prev = 0\n",
    "for i in range(test_events):\n",
    "    relevant_pred = predictions[prev:prev+min_window]\n",
    "    predictions_majority[i] = np.argmax(np.bincount(relevant_pred))\n",
    "    prev = prev+int(min_window)\n",
    "print('predictions after majority voting:')\n",
    "print(predictions_majority)\n",
    "predictions_majority = predictions_majority.astype(int)\n",
    "real_labels = np.array([0,0,1,1,2,2,3,3,4,4,5,5,6,6,7,7,8,8,9,9]*len(train_sessions))\n",
    "accuracy = np.mean(predictions_majority==real_labels)\n",
    "print('Real labels:')\n",
    "print(real_labels)\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "print('Test accuracy after majority voting:')\n",
    "print(accuracy)\n",
    "print('Confusion matrix:')\n",
    "plt.imshow(confusion_matrix(real_labels, predictions_majority))\n",
    "plt.colorbar()\n",
    "#plt.show()\n",
    "\n",
    "summary_name = 'summaries/subject_'+subject_num+'/task1_1s/summary_task1_1s_'+subject_num+'_pos'+position+'_type_KNN_run'+run\n",
    "f = open(summary_name+'.txt', 'w')\n",
    "f.write('Run summary: task 1, subject number '+subject_num+', position '+position+', algorithm type: KNN, run '+run+'\\n')\n",
    "f.write('sessions: ')\n",
    "for s_ind, s in enumerate(train_sessions):\n",
    "  if s_ind != len(train_sessions) - 1:\n",
    "    f.write(s+', ')\n",
    "  else:\n",
    "    f.write(s+'\\n')\n",
    "f.write('position: '+position+'\\n')\n",
    "f.write('k: '+str(k_KNN)+'\\n')\n",
    "f.write('Single frame accuracy: '+str(single_acc)+'\\n')\n",
    "f.write('Accuracy after majority voting: '+str(accuracy)+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6ItdxeS6P53"
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V89y37ly8p8s"
   },
   "source": [
    "**Task 1** - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2DMSvFuBrt5"
   },
   "outputs": [],
   "source": [
    "use_hmm = False\n",
    "run = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 658
    },
    "executionInfo": {
     "elapsed": 1068,
     "status": "ok",
     "timestamp": 1654334045468,
     "user": {
      "displayName": "Adi Ben-Ari",
      "userId": "15772198121198769105"
     },
     "user_tz": -180
    },
    "id": "_xqIJp4k8w4n",
    "outputId": "005b3871-4ab4-46d3-960d-969a0240f3e8"
   },
   "outputs": [],
   "source": [
    "c = 100\n",
    "\n",
    "# Read the files:\n",
    "train_data = None\n",
    "train_labels = None\n",
    "test_data = None\n",
    "test_labels = None\n",
    "if use_hmm:\n",
    "  name_start = 'hmm_generated_files/subject_'+subject_num+'/gf_'+subject_num+'_pos'+position+'_s'\n",
    "  name_start_val = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'\n",
    "else:\n",
    "  name_start = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'\n",
    "  name_start_val = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'\n",
    "for filename in [(name_start+session+'_train_data.pkl', name_start+session+'_train_labels.pkl', name_start_val+session+'_test_data.pkl', name_start_val+session+'_test_labels.pkl') for session in train_sessions]:\n",
    "#for filename in [(pre+'_generated_train_data_10gest.pkl', pre+'_generated_train_labels_10gest.pkl', pre+'_val_data_10gest.pkl', pre+'_val_labels_10gest.pkl') for pre in ['grc1pos1', 'grc1pos2']]:\n",
    "    if filename[0] is not None:\n",
    "        data = pd.read_pickle(filename[0])\n",
    "        data = data.to_numpy()\n",
    "    else:\n",
    "        data = None\n",
    "    print('train data read!')\n",
    "    if filename[1] is not None:\n",
    "        labels = pd.read_pickle(filename[1])\n",
    "        labels = labels.to_numpy()\n",
    "    else:\n",
    "        labels = None\n",
    "    print('train labels read!')\n",
    "    if filename[2] is not None:\n",
    "        test_data_part = pd.read_pickle(filename[2])\n",
    "        test_data_part = test_data_part.to_numpy()\n",
    "    else:\n",
    "        test_data_part = None\n",
    "    print('test data read!')\n",
    "    if filename[3] is not None:\n",
    "        test_labels_part = pd.read_pickle(filename[3])\n",
    "        test_labels_part = test_labels_part.to_numpy()\n",
    "    else:\n",
    "        test_labels_part = None\n",
    "    print('test labels read!')\n",
    "\n",
    "    if train_data is not None and data is not None:\n",
    "        train_data = np.concatenate((train_data, data))\n",
    "        train_labels = np.concatenate((train_labels, labels))\n",
    "    if test_data is not None and test_data_part is not None:\n",
    "        test_data = np.concatenate((test_data, test_data_part))\n",
    "        test_labels = np.concatenate((test_labels, test_labels_part))\n",
    "    if train_data is None or test_data is None:\n",
    "        if data is not None and train_data is None:\n",
    "            train_data = copy.deepcopy(data)\n",
    "            train_labels = copy.deepcopy(labels)\n",
    "        if test_data_part is not None and test_data is None:\n",
    "            test_data = copy.deepcopy(test_data_part)\n",
    "            test_labels = copy.deepcopy(test_labels_part)\n",
    "\n",
    "#svm_classifier = pickle.load(open('svm_model.sav', 'rb'))\n",
    "svm_classifier = svm.SVC(C=c)\n",
    "svm_classifier.fit(train_data, train_labels)\n",
    "predictions = svm_classifier.predict(test_data).astype('int')\n",
    "single_acc = np.mean(predictions==test_labels.T)\n",
    "print('SVM single frame accuracy:')\n",
    "print(single_acc)\n",
    "SVM_model_name = 'weights/SVM/SVM_task1_1s_'+subject_num+'_pos'+position+'_run'+run\n",
    "pickle.dump(svm_classifier, open(SVM_model_name+'.sav', 'wb'))\n",
    "\n",
    "\n",
    "# majority voting:\n",
    "predictions_majority = np.zeros(test_events)\n",
    "prev = 0\n",
    "for i in range(test_events):\n",
    "    relevant_pred = predictions[prev:prev+min_window]\n",
    "    predictions_majority[i] = np.argmax(np.bincount(relevant_pred))\n",
    "    prev = prev+int(min_window)\n",
    "print('predictions after majority voting:')\n",
    "print(predictions_majority)\n",
    "predictions_majority = predictions_majority.astype(int)\n",
    "real_labels = np.array([0,0,1,1,2,2,3,3,4,4,5,5,6,6,7,7,8,8,9,9]*len(train_sessions))\n",
    "accuracy = np.mean(predictions_majority==real_labels)\n",
    "print('Real labels:')\n",
    "print(real_labels)\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "print('Test accuracy after majority voting:')\n",
    "print(accuracy)\n",
    "print('Confusion matrix:')\n",
    "plt.imshow(confusion_matrix(real_labels, predictions_majority))\n",
    "plt.colorbar()\n",
    "#plt.show()\n",
    "\n",
    "summary_name = 'summaries/subject_'+subject_num+'/task1_1s/summary_task1_1s_'+subject_num+'_pos'+position+'_type_SVM_run'+run\n",
    "f = open(summary_name+'.txt', 'w')\n",
    "f.write('Run summary: task 1, subject number '+subject_num+', position '+position+', algorithm type: SVM, run '+run+'\\n')\n",
    "f.write('sessions: ')\n",
    "for s_ind, s in enumerate(train_sessions):\n",
    "  if s_ind != len(train_sessions) - 1:\n",
    "    f.write(s+', ')\n",
    "  else:\n",
    "    f.write(s+'\\n')\n",
    "f.write('position: '+position+'\\n')\n",
    "f.write('C: '+str(c)+'\\n')\n",
    "f.write('model name: '+SVM_model_name+'.sav'+'\\n')\n",
    "f.write('single frame accuracy: '+str(single_acc)+'\\n')\n",
    "f.write('accuracy after majority voting: '+str(accuracy)+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96ogrpPAilx_"
   },
   "source": [
    "# Classification task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2YZiflQilyA"
   },
   "source": [
    "4 classification algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVmQIimv6y3M"
   },
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moJahdJyilyA"
   },
   "source": [
    "Training the CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pAKGDMGDilyB"
   },
   "outputs": [],
   "source": [
    "use_hmm = True\n",
    "run = '1'\n",
    "train_sessions = ['1', '2']\n",
    "subject_num = '004'\n",
    "train_positions = ['1', '2', '3']\n",
    "min_window = 11\n",
    "\n",
    "pos_session_list = []\n",
    "for s in train_sessions:\n",
    "  for p in train_positions:\n",
    "    pos_session_list.append((p, s))\n",
    "\n",
    "test_events = 20*len(pos_session_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 301512,
     "status": "ok",
     "timestamp": 1654266551695,
     "user": {
      "displayName": "Liron Ben-Ari",
      "userId": "04890134261687775011"
     },
     "user_tz": -180
    },
    "id": "oFPVUmSdilyB",
    "outputId": "2b4003e1-ca83-4042-8903-97f34cdde1a2"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Read the files:\n",
    "train_data = None\n",
    "train_labels = None\n",
    "test_data = None\n",
    "test_labels = None\n",
    "if use_hmm:\n",
    "  name_start = 'hmm_generated_files/subject_'+subject_num+'/gf_'+subject_num\n",
    "  name_start_val = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "else:\n",
    "  name_start = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "  name_start_val = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "for filename in [(name_start+'_pos'+position+'_s'+session+'_train_data.pkl', name_start+'_pos'+position+'_s'+session+'_train_labels.pkl', name_start_val+'_pos'+position+'_s'+session+'_val_data.pkl', name_start_val+'_pos'+position+'_s'+session+'_val_labels.pkl') for position, session in pos_session_list]:\n",
    "#for filename in [(pre+'_generated_train_data_10gest.pkl', pre+'_generated_train_labels_10gest.pkl', pre+'_val_data_10gest.pkl', pre+'_val_labels_10gest.pkl') for pre in ['grc1pos1', 'grc1pos2']]:\n",
    "    if filename[0] is not None:\n",
    "        data = pd.read_pickle(filename[0])\n",
    "        data = data.to_numpy()\n",
    "    else:\n",
    "        data = None\n",
    "    print('train data read!')\n",
    "    if filename[1] is not None:\n",
    "        labels = pd.read_pickle(filename[1])\n",
    "        labels = labels.to_numpy()\n",
    "    else:\n",
    "        labels = None\n",
    "    print('train labels read!')\n",
    "    if filename[2] is not None:\n",
    "        test_data_part = pd.read_pickle(filename[2])\n",
    "        test_data_part = test_data_part.to_numpy()\n",
    "    else:\n",
    "        test_data_part = None\n",
    "    print('test data read!')\n",
    "    if filename[3] is not None:\n",
    "        test_labels_part = pd.read_pickle(filename[3])\n",
    "        test_labels_part = test_labels_part.to_numpy()\n",
    "    else:\n",
    "        test_labels_part = None\n",
    "    print('test labels read!')\n",
    "\n",
    "    if train_data is not None and data is not None:\n",
    "        train_data = np.concatenate((train_data, data))\n",
    "        train_labels = np.concatenate((train_labels, labels))\n",
    "    if test_data is not None and test_data_part is not None:\n",
    "        test_data = np.concatenate((test_data, test_data_part))\n",
    "        test_labels = np.concatenate((test_labels, test_labels_part))\n",
    "    if train_data is None or test_data is None:\n",
    "        if data is not None and train_data is None:\n",
    "            train_data = copy.deepcopy(data)\n",
    "            train_labels = copy.deepcopy(labels)\n",
    "        if test_data_part is not None and test_data is None:\n",
    "            test_data = copy.deepcopy(test_data_part)\n",
    "            test_labels = copy.deepcopy(test_labels_part)\n",
    "\n",
    "train_data = np.reshape(train_data, (train_data.shape[0], 4, 4))\n",
    "test_data = np.reshape(test_data, (test_data.shape[0], 4, 4))\n",
    "print('shapes of train data, train labels, test data, test labels:')\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n",
    "per = np.random.permutation(len(train_data))\n",
    "train_data = train_data[per, :]\n",
    "train_labels = train_labels[per]\n",
    "# Data standartization:\n",
    "train_data = train_data - np.mean(train_data, axis=0)\n",
    "train_data = train_data / np.std(train_data, axis=0)\n",
    "test_data = test_data - np.mean(test_data, axis=0)\n",
    "test_data = test_data / np.std(test_data, axis=0)\n",
    "\n",
    "train_data = tf.expand_dims(train_data, axis=-1)\n",
    "test_data = tf.expand_dims(test_data, axis=-1)\n",
    "\n",
    "# Hyper-parameters:\n",
    "if use_hmm:\n",
    "  algo_type='CNN+HMM'\n",
    "else:\n",
    "  algo_type='CNN'\n",
    "reg_strength = 0.0001\n",
    "lr = 5e-4\n",
    "d1 = 0.3\n",
    "epochs_num = 1500 #1500\n",
    "batch_size = 10000\n",
    "#min_window = real_min_window\n",
    "print('min_window='+str(min_window))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(2, 2, padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_strength)),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(4, 2, padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_strength)),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(40, kernel_regularizer=tf.keras.regularizers.l2(reg_strength)),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(d1),\n",
    "    tf.keras.layers.Dense(20, kernel_regularizer=tf.keras.regularizers.l2(reg_strength)),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(d1),\n",
    "    tf.keras.layers.Dense(10, kernel_regularizer=tf.keras.regularizers.l2(reg_strength), name='new'),\n",
    "    tf.keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "def scheduler(epoch):\n",
    "  if epoch < 600:\n",
    "    return lr\n",
    "  else:\n",
    "    return lr\n",
    "\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_data, train_labels, epochs=epochs_num,batch_size=batch_size, callbacks=[callback])\n",
    "model.save_weights('weights/CNN/weights_task2_'+subject_num+'_run'+run)\n",
    "#model.load_weights('weights/CNN/weights_task2_'+subject_num+'_run'+run)\n",
    "test_loss, test_acc = model.evaluate(test_data,  test_labels)\n",
    "print('\\nSingle frame based validation accuracy:', test_acc)\n",
    "predictions = np.argmax(model.predict(test_data), axis=1)\n",
    "print('Network predictions:')\n",
    "print(predictions)\n",
    "print('Real labels:')\n",
    "print(test_labels.T)\n",
    "\n",
    "# majority voting:\n",
    "predictions_majority = np.zeros(test_events)\n",
    "prev = 0\n",
    "for i in range(test_events):\n",
    "    relevant_pred = predictions[prev:prev+min_window]\n",
    "    predictions_majority[i] = np.argmax(np.bincount(relevant_pred))\n",
    "    prev = prev+int(min_window)\n",
    "print('Network predictions after majority voting:')\n",
    "print(predictions_majority)\n",
    "predictions_majority = predictions_majority.astype(int)\n",
    "real_labels = np.array([0,0,1,1,2,2,3,3,4,4,5,5,6,6,7,7,8,8,9,9]*len(pos_session_list))\n",
    "accuracy = np.mean(predictions_majority==real_labels)\n",
    "print('Real labels:')\n",
    "print(real_labels)\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].plot(history.history['accuracy'])\n",
    "axs[0].set(title='Training Accuracy', xlabel='epochs', ylabel='accuracy')\n",
    "axs[1].plot(history.history['loss'])\n",
    "axs[1].set(title='Training Loss', xlabel='epochs', ylabel='loss')\n",
    "#model.summary()\n",
    "plt.show()\n",
    "\n",
    "print('Validation accuracy after majority voting:')\n",
    "print(accuracy)\n",
    "print('Confusion matrix:')\n",
    "plt.imshow(confusion_matrix(real_labels, predictions_majority))\n",
    "plt.colorbar()\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYwN8Pq9ilyD"
   },
   "source": [
    "**Task 1:** testing the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4985,
     "status": "ok",
     "timestamp": 1654354047156,
     "user": {
      "displayName": "Adi Ben-Ari",
      "userId": "15772198121198769105"
     },
     "user_tz": -180
    },
    "id": "QcwPP4TTilyD",
    "outputId": "f55ee691-6a5a-4f5a-9b1d-58910535893c"
   },
   "outputs": [],
   "source": [
    "# Read the files:\n",
    "train_data = None\n",
    "train_labels = None\n",
    "test_data = None\n",
    "test_labels = None\n",
    "name_start = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "name_start_val = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "for filename in [(name_start_val+'_pos'+position+'_s'+session+'_test_data.pkl', name_start_val+'_pos'+position+'_s'+session+'_test_labels.pkl') for position, session in pos_session_list]:    \n",
    "    if filename[0] is not None:\n",
    "        test_data_part = pd.read_pickle(filename[0])\n",
    "        test_data_part = test_data_part.to_numpy()\n",
    "    else:\n",
    "        test_data_part = None\n",
    "    print('test data read!')\n",
    "    if filename[1] is not None:\n",
    "        test_labels_part = pd.read_pickle(filename[1])\n",
    "        test_labels_part = test_labels_part.to_numpy()\n",
    "    else:\n",
    "        test_labels_part = None\n",
    "    print('test labels read!')\n",
    "\n",
    "    if train_data is not None and data is not None:\n",
    "        train_data = np.concatenate((train_data, data))\n",
    "        train_labels = np.concatenate((train_labels, labels))\n",
    "    if test_data is not None and test_data_part is not None:\n",
    "        test_data = np.concatenate((test_data, test_data_part))\n",
    "        test_labels = np.concatenate((test_labels, test_labels_part))\n",
    "    if train_data is None or test_data is None:\n",
    "        if data is not None and train_data is None:\n",
    "            train_data = copy.deepcopy(data)\n",
    "            train_labels = copy.deepcopy(labels)\n",
    "        if test_data_part is not None and test_data is None:\n",
    "            test_data = copy.deepcopy(test_data_part)\n",
    "            test_labels = copy.deepcopy(test_labels_part)\n",
    "\n",
    "test_data = np.reshape(test_data, (test_data.shape[0], 4, 4))\n",
    "print('shapes of test data, test labels:')\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "# Data standartization:\n",
    "test_data = test_data - np.mean(test_data, axis=0)\n",
    "test_data = test_data / np.std(test_data, axis=0)\n",
    "\n",
    "test_data = tf.expand_dims(test_data, axis=-1)\n",
    "\n",
    "model.load_weights('weights/CNN/weights_task2_'+subject_num+'_run'+run)\n",
    "test_loss, test_acc = model.evaluate(test_data,  test_labels)\n",
    "print('\\nSingle frame based test accuracy:', test_acc)\n",
    "predictions = np.argmax(model.predict(test_data), axis=1)\n",
    "print('Network predictions:')\n",
    "print(predictions)\n",
    "print('Real labels:')\n",
    "print(test_labels.T)\n",
    "\n",
    "# majority voting:\n",
    "predictions_majority = np.zeros(test_events)\n",
    "prev = 0\n",
    "for i in range(test_events):\n",
    "    relevant_pred = predictions[prev:prev+min_window]\n",
    "    predictions_majority[i] = np.argmax(np.bincount(relevant_pred))\n",
    "    prev = prev+int(min_window)\n",
    "print('Network predictions after majority voting:')\n",
    "print(predictions_majority)\n",
    "predictions_majority = predictions_majority.astype(int)\n",
    "real_labels = np.array([0,0,1,1,2,2,3,3,4,4,5,5,6,6,7,7,8,8,9,9]*len(pos_session_list))\n",
    "accuracy = np.mean(predictions_majority==real_labels)\n",
    "print('Real labels:')\n",
    "print(real_labels)\n",
    "\n",
    "print('Test accuracy after majority voting:')\n",
    "print(accuracy)\n",
    "print('Confusion matrix:')\n",
    "mat = confusion_matrix(real_labels, predictions_majority)\n",
    "print(repr(mat))\n",
    "plt.imshow(mat)\n",
    "#plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "folder_path = 'summaries/subject_'+subject_num+'/task2'\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "summary_name = 'summaries/subject_'+subject_num+'/task2/summary_task2_'+subject_num+'_type_'+algo_type+'_run'+run\n",
    "f = open(summary_name+'.txt', 'w')\n",
    "f.write('Run summary: task 2, subject number '+subject_num+', algorithm type: '+algo_type+', run '+run+'\\n')\n",
    "f.write('sessions: ')\n",
    "for s_ind, s in enumerate(train_sessions):\n",
    "  if s_ind != len(train_sessions) - 1:\n",
    "    f.write(s+', ')\n",
    "  else:\n",
    "    f.write(s+'\\n')\n",
    "f.write('positions: ')\n",
    "for p_ind, p in enumerate(train_positions):\n",
    "  if p_ind != len(train_positions) - 1:\n",
    "    f.write(p+', ')\n",
    "  else:\n",
    "    f.write(p+'\\n')   \n",
    "f.write('regularization strength: '+str(reg_strength)+'\\n')\n",
    "f.write('learning rate: '+str(lr)+'\\n')\n",
    "f.write('dropout: '+str(d1)+'\\n')\n",
    "f.write('number of epochs: '+str(epochs_num)+'\\n')\n",
    "f.write('batch size: '+str(batch_size)+'\\n')\n",
    "f.write('minimal window length (in samples): '+str(min_window)+'\\n')\n",
    "f.write('Single frame based validation accuracy: '+str(test_acc)+'\\n')\n",
    "f.write('Test accuracy after majority voting: '+str(accuracy)+'\\n')\n",
    "f.write('weights file: weights_task2_'+subject_num+'_run'+run+'\\n')\n",
    "f.close()\n",
    "\n",
    "confusion_matrix_df = pd.DataFrame(data=mat)\n",
    "confusion_matrix_df.to_pickle(folder_path+'/CNN_mat.pkl', protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fS9nzn4h617R"
   },
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVhlsHkyilyE"
   },
   "source": [
    "Training the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q965eJfEilyE"
   },
   "outputs": [],
   "source": [
    "use_hmm = True\n",
    "run = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 86188,
     "status": "ok",
     "timestamp": 1654266638409,
     "user": {
      "displayName": "Liron Ben-Ari",
      "userId": "04890134261687775011"
     },
     "user_tz": -180
    },
    "id": "IpKk8s6vilyF",
    "outputId": "6de69744-2feb-48a0-8989-0b771cdc28ef"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Read the files:\n",
    "train_data = None\n",
    "train_labels = None\n",
    "test_data = None\n",
    "test_labels = None\n",
    "if use_hmm:\n",
    "  name_start = 'hmm_generated_files/subject_'+subject_num+'/gf_'+subject_num\n",
    "  name_start_val = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "else:\n",
    "  name_start = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "  name_start_val = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "for filename in [(name_start+'_pos'+position+'_s'+session+'_train_data.pkl', name_start+'_pos'+position+'_s'+session+'_train_labels.pkl', name_start_val+'_pos'+position+'_s'+session+'_val_data.pkl', name_start_val+'_pos'+position+'_s'+session+'_val_labels.pkl') for position, session in pos_session_list]:\n",
    "#for filename in [(pre+'_generated_train_data_10gest.pkl', pre+'_generated_train_labels_10gest.pkl', pre+'_val_data_10gest.pkl', pre+'_val_labels_10gest.pkl') for pre in ['grc1pos1', 'grc1pos2']]:\n",
    "    if filename[0] is not None:\n",
    "        data = pd.read_pickle(filename[0])\n",
    "        data = data.to_numpy()\n",
    "    else:\n",
    "        data = None\n",
    "    print('train data read!')\n",
    "    if filename[1] is not None:\n",
    "        labels = pd.read_pickle(filename[1])\n",
    "        labels = labels.to_numpy()\n",
    "    else:\n",
    "        labels = None\n",
    "    print('train labels read!')\n",
    "    if filename[2] is not None:\n",
    "        test_data_part = pd.read_pickle(filename[2])\n",
    "        test_data_part = test_data_part.to_numpy()\n",
    "    else:\n",
    "        test_data_part = None\n",
    "    print('test data read!')\n",
    "    if filename[3] is not None:\n",
    "        test_labels_part = pd.read_pickle(filename[3])\n",
    "        test_labels_part = test_labels_part.to_numpy()\n",
    "    else:\n",
    "        test_labels_part = None\n",
    "    print('test labels read!')\n",
    "\n",
    "    if train_data is not None and data is not None:\n",
    "        train_data = np.concatenate((train_data, data))\n",
    "        train_labels = np.concatenate((train_labels, labels))\n",
    "    if test_data is not None and test_data_part is not None:\n",
    "        test_data = np.concatenate((test_data, test_data_part))\n",
    "        test_labels = np.concatenate((test_labels, test_labels_part))\n",
    "    if train_data is None or test_data is None:\n",
    "        if data is not None and train_data is None:\n",
    "            train_data = copy.deepcopy(data)\n",
    "            train_labels = copy.deepcopy(labels)\n",
    "        if test_data_part is not None and test_data is None:\n",
    "            test_data = copy.deepcopy(test_data_part)\n",
    "            test_labels = copy.deepcopy(test_labels_part)\n",
    "\n",
    "\n",
    "# Hyper-parameters:\n",
    "if use_hmm:\n",
    "  algo_type='RNN+HMM'\n",
    "else:\n",
    "  algo_type='RNN'\n",
    "reg_strength = 0.0001\n",
    "lr = 1e-2\n",
    "d1 = 0.1\n",
    "#min_angle = 1/18\n",
    "#max_shift_h = 0.05\n",
    "#max_shift_w = 0.05\n",
    "epochs_num = 1000 #1500\n",
    "batch_size = 150\n",
    "print('min_window='+str(min_window))\n",
    "\n",
    "\n",
    "windows_num = min_window\n",
    "reshaped_train_data = np.zeros((int(train_data.shape[0]/windows_num), windows_num, 16))\n",
    "reshaped_test_data = np.zeros((int(test_data.shape[0]/windows_num), windows_num, 16))\n",
    "for i in range(reshaped_train_data.shape[0]):\n",
    "    reshaped_train_data[i,:,:] = train_data[windows_num*i:windows_num*(i+1)]\n",
    "for i in range(reshaped_test_data.shape[0]):\n",
    "    reshaped_test_data[i,:,:] = test_data[windows_num*i:windows_num*(i+1)]\n",
    "train_data = reshaped_train_data\n",
    "test_data = reshaped_test_data\n",
    "train_labels = train_labels[::windows_num]\n",
    "test_labels = test_labels[::windows_num]\n",
    "print('shapes:')\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "\n",
    "per = np.random.permutation(len(train_data))\n",
    "train_data = train_data[per, :]\n",
    "train_labels = train_labels[per]\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(12, dropout=d1, input_shape=(windows_num,16), name='lstm2'),\n",
    "    tf.keras.layers.Dense(10, kernel_regularizer=tf.keras.regularizers.l2(reg_strength), name='new'),\n",
    "    tf.keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "def scheduler(epoch):\n",
    "  if epoch < 600:\n",
    "    return lr\n",
    "  else:\n",
    "    return lr\n",
    "\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_data, train_labels, epochs=epochs_num,batch_size=batch_size, callbacks=[callback])\n",
    "model.save_weights('weights/RNN/weights_task2_'+subject_num+'_run'+run)\n",
    "#model.load_weights('weights/RNN/weights_task2_'+subject_num+'_pos'+position+'_run'+run)\n",
    "test_loss, test_acc = model.evaluate(test_data,  test_labels)#, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "\n",
    "predictions = np.argmax(model.predict(test_data), axis=1)\n",
    "cmat = confusion_matrix(test_labels, predictions)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].plot(history.history['accuracy'])\n",
    "axs[0].set(title='Training Accuracy', xlabel='epochs', ylabel='accuracy')\n",
    "axs[1].plot(history.history['loss'])\n",
    "axs[1].set(title='Training Loss', xlabel='epochs', ylabel='loss')\n",
    "#model.summary()\n",
    "plt.figure()\n",
    "print('Confusion matrix:')\n",
    "plt.imshow(cmat)\n",
    "plt.colorbar()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VojJP_GuilyG"
   },
   "source": [
    "Testing the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2002,
     "status": "ok",
     "timestamp": 1654266640375,
     "user": {
      "displayName": "Liron Ben-Ari",
      "userId": "04890134261687775011"
     },
     "user_tz": -180
    },
    "id": "5Pdg9hZgilyH",
    "outputId": "fe4fb1d0-b7f3-468d-a02e-3989e004cfce"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Read the files:\n",
    "train_data = None\n",
    "train_labels = None\n",
    "test_data = None\n",
    "test_labels = None\n",
    "if use_hmm:\n",
    "  name_start = 'hmm_generated_files/subject_'+subject_num+'/gf_'+subject_num\n",
    "  name_start_val = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "else:\n",
    "  name_start = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "  name_start_val = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "for filename in [(name_start+'_pos'+position+'_s'+session+'_train_data.pkl', name_start+'_pos'+position+'_s'+session+'_train_labels.pkl', name_start_val+'_pos'+position+'_s'+session+'_val_data.pkl', name_start_val+'_pos'+position+'_s'+session+'_val_labels.pkl') for position, session in pos_session_list]:\n",
    "#for filename in [(pre+'_generated_train_data_10gest.pkl', pre+'_generated_train_labels_10gest.pkl', pre+'_val_data_10gest.pkl', pre+'_val_labels_10gest.pkl') for pre in ['grc1pos1', 'grc1pos2']]:\n",
    "    if filename[0] is not None:\n",
    "        data = pd.read_pickle(filename[0])\n",
    "        data = data.to_numpy()\n",
    "    else:\n",
    "        data = None\n",
    "    print('train data read!')\n",
    "    if filename[1] is not None:\n",
    "        labels = pd.read_pickle(filename[1])\n",
    "        labels = labels.to_numpy()\n",
    "    else:\n",
    "        labels = None\n",
    "    print('train labels read!')\n",
    "    if filename[2] is not None:\n",
    "        test_data_part = pd.read_pickle(filename[2])\n",
    "        test_data_part = test_data_part.to_numpy()\n",
    "    else:\n",
    "        test_data_part = None\n",
    "    print('test data read!')\n",
    "    if filename[3] is not None:\n",
    "        test_labels_part = pd.read_pickle(filename[3])\n",
    "        test_labels_part = test_labels_part.to_numpy()\n",
    "    else:\n",
    "        test_labels_part = None\n",
    "    print('test labels read!')\n",
    "\n",
    "    if train_data is not None and data is not None:\n",
    "        train_data = np.concatenate((train_data, data))\n",
    "        train_labels = np.concatenate((train_labels, labels))\n",
    "    if test_data is not None and test_data_part is not None:\n",
    "        test_data = np.concatenate((test_data, test_data_part))\n",
    "        test_labels = np.concatenate((test_labels, test_labels_part))\n",
    "    if train_data is None or test_data is None:\n",
    "        if data is not None and train_data is None:\n",
    "            train_data = copy.deepcopy(data)\n",
    "            train_labels = copy.deepcopy(labels)\n",
    "        if test_data_part is not None and test_data is None:\n",
    "            test_data = copy.deepcopy(test_data_part)\n",
    "            test_labels = copy.deepcopy(test_labels_part)\n",
    "\n",
    "\n",
    "# Hyper-parameters:\n",
    "if use_hmm:\n",
    "  algo_type='RNN+HMM'\n",
    "else:\n",
    "  algo_type='RNN'\n",
    "reg_strength = 0.0001\n",
    "lr = 1e-2\n",
    "d1 = 0.1\n",
    "#min_angle = 1/18\n",
    "#max_shift_h = 0.05\n",
    "#max_shift_w = 0.05\n",
    "epochs_num = 2000 #1500\n",
    "batch_size = 500\n",
    "print('min_window='+str(min_window))\n",
    "\n",
    "windows_num = min_window\n",
    "reshaped_train_data = np.zeros((int(train_data.shape[0]/windows_num), windows_num, 16))\n",
    "reshaped_test_data = np.zeros((int(test_data.shape[0]/windows_num), windows_num, 16))\n",
    "for i in range(reshaped_train_data.shape[0]):\n",
    "    reshaped_train_data[i,:,:] = train_data[windows_num*i:windows_num*(i+1)]\n",
    "for i in range(reshaped_test_data.shape[0]):\n",
    "    reshaped_test_data[i,:,:] = test_data[windows_num*i:windows_num*(i+1)]\n",
    "train_data = reshaped_train_data\n",
    "test_data = reshaped_test_data\n",
    "train_labels = train_labels[::windows_num]\n",
    "test_labels = test_labels[::windows_num]\n",
    "print('shapes:')\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "\n",
    "per = np.random.permutation(len(train_data))\n",
    "train_data = train_data[per, :]\n",
    "train_labels = train_labels[per]\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(12, dropout=d1, input_shape=(windows_num,16), name='lstm2'),\n",
    "    tf.keras.layers.Dense(10, kernel_regularizer=tf.keras.regularizers.l2(reg_strength), name='new'),\n",
    "    tf.keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "def scheduler(epoch):\n",
    "  if epoch < 600:\n",
    "    return lr\n",
    "  else:\n",
    "    return lr\n",
    "\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#history = model.fit(train_data, train_labels, epochs=epochs_num,batch_size=batch_size, callbacks=[callback])\n",
    "#model.save_weights('weights/RNN/weights_task1_'+subject_num+'_pos'+position+'_run'+run)\n",
    "model.load_weights('weights/RNN/weights_task2_'+subject_num+'_run'+run)\n",
    "test_loss, test_acc = model.evaluate(test_data,  test_labels)#, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "\n",
    "predictions = np.argmax(model.predict(test_data), axis=1)\n",
    "cmat = confusion_matrix(test_labels, predictions)\n",
    "\n",
    "folder_path = 'summaries/subject_'+subject_num+'/task2'\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "summary_name = 'summaries/subject_'+subject_num+'/task2/summary_task2_'+subject_num+'_type_'+algo_type+'_run'+run\n",
    "f = open(summary_name+'.txt', 'w')\n",
    "f.write('Run summary: task 2, subject number '+subject_num+', algorithm type: '+algo_type+', run '+run+'\\n')\n",
    "f.write('sessions: ')\n",
    "for s_ind, s in enumerate(train_sessions):\n",
    "  if s_ind != len(train_sessions) - 1:\n",
    "    f.write(s+', ')\n",
    "  else:\n",
    "    f.write(s+'\\n')\n",
    "f.write('positions: ')\n",
    "for p_ind, p in enumerate(train_positions):\n",
    "  if p_ind != len(train_positions) - 1:\n",
    "    f.write(p+', ')\n",
    "  else:\n",
    "    f.write(p+'\\n')   \n",
    "f.write('regularization strength: '+str(reg_strength)+'\\n')\n",
    "f.write('learning rate: '+str(lr)+'\\n')\n",
    "f.write('dropout: '+str(d1)+'\\n')\n",
    "f.write('number of epochs: '+str(epochs_num)+'\\n')\n",
    "f.write('batch size: '+str(batch_size)+'\\n')\n",
    "f.write('minimal window length (in samples): '+str(min_window)+'\\n')\n",
    "f.write('Test accuracy after majority voting: '+str(test_acc)+'\\n')\n",
    "f.write('weights file: weights_task2_'+subject_num+'_run'+run+'\\n')\n",
    "f.close()\n",
    "\n",
    "confusion_matrix_df = pd.DataFrame(data=mat)\n",
    "confusion_matrix_df.to_pickle(folder_path+'/RNN_mat.pkl', protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkitajPX665Y"
   },
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJOeGmF5ilyI"
   },
   "source": [
    "**Task 1** - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jEs2sU3ilyJ"
   },
   "outputs": [],
   "source": [
    "use_hmm = False\n",
    "run = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 668,
     "status": "ok",
     "timestamp": 1654266641035,
     "user": {
      "displayName": "Liron Ben-Ari",
      "userId": "04890134261687775011"
     },
     "user_tz": -180
    },
    "id": "NkCw9OIKilyJ",
    "outputId": "350d53a5-1aa1-45f3-f0d6-47735a4a5185"
   },
   "outputs": [],
   "source": [
    "k_KNN = 1\n",
    "\n",
    "# Read the files:\n",
    "train_data = None\n",
    "train_labels = None\n",
    "test_data = None\n",
    "test_labels = None\n",
    "if use_hmm:\n",
    "  name_start = 'hmm_generated_files/subject_'+subject_num+'/gf_'+subject_num\n",
    "  name_start_val = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "else:\n",
    "  name_start = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "  name_start_val = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "for filename in [(name_start+'_pos'+position+'_s'+session+'_train_data.pkl', name_start+'_pos'+position+'_s'+session+'_train_labels.pkl', name_start_val+'_pos'+position+'_s'+session+'_val_data.pkl', name_start_val+'_pos'+position+'_s'+session+'_val_labels.pkl') for position, session in pos_session_list]:\n",
    "#for filename in [(pre+'_generated_train_data_10gest.pkl', pre+'_generated_train_labels_10gest.pkl', pre+'_val_data_10gest.pkl', pre+'_val_labels_10gest.pkl') for pre in ['grc1pos1', 'grc1pos2']]:\n",
    "    if filename[0] is not None:\n",
    "        data = pd.read_pickle(filename[0])\n",
    "        data = data.to_numpy()\n",
    "    else:\n",
    "        data = None\n",
    "    print('train data read!')\n",
    "    if filename[1] is not None:\n",
    "        labels = pd.read_pickle(filename[1])\n",
    "        labels = labels.to_numpy()\n",
    "    else:\n",
    "        labels = None\n",
    "    print('train labels read!')\n",
    "    if filename[2] is not None:\n",
    "        test_data_part = pd.read_pickle(filename[2])\n",
    "        test_data_part = test_data_part.to_numpy()\n",
    "    else:\n",
    "        test_data_part = None\n",
    "    print('test data read!')\n",
    "    if filename[3] is not None:\n",
    "        test_labels_part = pd.read_pickle(filename[3])\n",
    "        test_labels_part = test_labels_part.to_numpy()\n",
    "    else:\n",
    "        test_labels_part = None\n",
    "    print('test labels read!')\n",
    "\n",
    "    if train_data is not None and data is not None:\n",
    "        train_data = np.concatenate((train_data, data))\n",
    "        train_labels = np.concatenate((train_labels, labels))\n",
    "    if test_data is not None and test_data_part is not None:\n",
    "        test_data = np.concatenate((test_data, test_data_part))\n",
    "        test_labels = np.concatenate((test_labels, test_labels_part))\n",
    "    if train_data is None or test_data is None:\n",
    "        if data is not None and train_data is None:\n",
    "            train_data = copy.deepcopy(data)\n",
    "            train_labels = copy.deepcopy(labels)\n",
    "        if test_data_part is not None and test_data is None:\n",
    "            test_data = copy.deepcopy(test_data_part)\n",
    "            test_labels = copy.deepcopy(test_labels_part)\n",
    "\n",
    "KNN = KNeighborsClassifier(n_neighbors=k_KNN)\n",
    "KNN.fit(train_data, train_labels)\n",
    "predictions = KNN.predict(test_data).astype('int')\n",
    "single_acc = np.mean(predictions==test_labels.T)\n",
    "print('KNN single frame accuracy: '+str(single_acc))\n",
    "\n",
    "# majority voting:\n",
    "predictions_majority = np.zeros(test_events)\n",
    "prev = 0\n",
    "for i in range(test_events):\n",
    "    relevant_pred = predictions[prev:prev+min_window]\n",
    "    predictions_majority[i] = np.argmax(np.bincount(relevant_pred))\n",
    "    prev = prev+int(min_window)\n",
    "print('predictions after majority voting:')\n",
    "print(predictions_majority)\n",
    "predictions_majority = predictions_majority.astype(int)\n",
    "real_labels = np.array([0,0,1,1,2,2,3,3,4,4,5,5,6,6,7,7,8,8,9,9]*len(pos_session_list))\n",
    "accuracy = np.mean(predictions_majority==real_labels)\n",
    "print('Real labels:')\n",
    "print(real_labels)\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "print('Test accuracy after majority voting:')\n",
    "print(accuracy)\n",
    "print('Confusion matrix:')\n",
    "plt.imshow(confusion_matrix(real_labels, predictions_majority))\n",
    "plt.colorbar()\n",
    "#plt.show()\n",
    "summary_name = 'summaries/subject_'+subject_num+'/task2/summary_task2_'+subject_num+'_type_KNN_run'+run\n",
    "f = open(summary_name+'.txt', 'w')\n",
    "f.write('Run summary: task 2, subject number '+subject_num+', algorithm type: KNN, run '+run+'\\n')\n",
    "f.write('sessions: ')\n",
    "for s_ind, s in enumerate(train_sessions):\n",
    "  if s_ind != len(train_sessions) - 1:\n",
    "    f.write(s+', ')\n",
    "  else:\n",
    "    f.write(s+'\\n')\n",
    "f.write('positions: ')\n",
    "for p_ind, p in enumerate(train_positions):\n",
    "  if p_ind != len(train_positions) - 1:\n",
    "    f.write(p+', ')\n",
    "  else:\n",
    "    f.write(p+'\\n')   \n",
    "f.write('k: '+str(k_KNN)+'\\n')\n",
    "f.write('Single frame accuracy: '+str(single_acc)+'\\n')\n",
    "f.write('Accuracy after majority voting: '+str(accuracy)+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5CC0i9269bJ"
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOAdOsB9ilyK"
   },
   "source": [
    "**Task 2** - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QjcMv_mAilyK"
   },
   "outputs": [],
   "source": [
    "use_hmm = False\n",
    "run = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1121,
     "status": "ok",
     "timestamp": 1654266642565,
     "user": {
      "displayName": "Liron Ben-Ari",
      "userId": "04890134261687775011"
     },
     "user_tz": -180
    },
    "id": "KP9KLJXvilyL",
    "outputId": "530fb4d7-6d7d-411e-e492-b54b2d914233"
   },
   "outputs": [],
   "source": [
    "c = 100\n",
    "\n",
    "# Read the files:\n",
    "train_data = None\n",
    "train_labels = None\n",
    "test_data = None\n",
    "test_labels = None\n",
    "if use_hmm:\n",
    "  name_start = 'hmm_generated_files/subject_'+subject_num+'/gf_'+subject_num\n",
    "  name_start_val = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "else:\n",
    "  name_start = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "  name_start_val = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "for filename in [(name_start+'_pos'+position+'_s'+session+'_train_data.pkl', name_start+'_pos'+position+'_s'+session+'_train_labels.pkl', name_start_val+'_pos'+position+'_s'+session+'_val_data.pkl', name_start_val+'_pos'+position+'_s'+session+'_val_labels.pkl') for position, session in pos_session_list]:\n",
    "#for filename in [(pre+'_generated_train_data_10gest.pkl', pre+'_generated_train_labels_10gest.pkl', pre+'_val_data_10gest.pkl', pre+'_val_labels_10gest.pkl') for pre in ['grc1pos1', 'grc1pos2']]:\n",
    "    if filename[0] is not None:\n",
    "        data = pd.read_pickle(filename[0])\n",
    "        data = data.to_numpy()\n",
    "    else:\n",
    "        data = None\n",
    "    print('train data read!')\n",
    "    if filename[1] is not None:\n",
    "        labels = pd.read_pickle(filename[1])\n",
    "        labels = labels.to_numpy()\n",
    "    else:\n",
    "        labels = None\n",
    "    print('train labels read!')\n",
    "    if filename[2] is not None:\n",
    "        test_data_part = pd.read_pickle(filename[2])\n",
    "        test_data_part = test_data_part.to_numpy()\n",
    "    else:\n",
    "        test_data_part = None\n",
    "    print('test data read!')\n",
    "    if filename[3] is not None:\n",
    "        test_labels_part = pd.read_pickle(filename[3])\n",
    "        test_labels_part = test_labels_part.to_numpy()\n",
    "    else:\n",
    "        test_labels_part = None\n",
    "    print('test labels read!')\n",
    "\n",
    "    if train_data is not None and data is not None:\n",
    "        train_data = np.concatenate((train_data, data))\n",
    "        train_labels = np.concatenate((train_labels, labels))\n",
    "    if test_data is not None and test_data_part is not None:\n",
    "        test_data = np.concatenate((test_data, test_data_part))\n",
    "        test_labels = np.concatenate((test_labels, test_labels_part))\n",
    "    if train_data is None or test_data is None:\n",
    "        if data is not None and train_data is None:\n",
    "            train_data = copy.deepcopy(data)\n",
    "            train_labels = copy.deepcopy(labels)\n",
    "        if test_data_part is not None and test_data is None:\n",
    "            test_data = copy.deepcopy(test_data_part)\n",
    "            test_labels = copy.deepcopy(test_labels_part)\n",
    "\n",
    "#svm_classifier = pickle.load(open('svm_model.sav', 'rb'))\n",
    "svm_classifier = svm.SVC(C=c)\n",
    "svm_classifier.fit(train_data, train_labels)\n",
    "predictions = svm_classifier.predict(test_data).astype('int')\n",
    "single_acc = np.mean(predictions==test_labels.T)\n",
    "print('SVM single frame accuracy:')\n",
    "print(single_acc)\n",
    "SVM_model_name = 'weights/SVM/SVM_task2_'+subject_num+'_run'+run\n",
    "pickle.dump(svm_classifier, open(SVM_model_name+'.sav', 'wb'))\n",
    "\n",
    "\n",
    "# majority voting:\n",
    "predictions_majority = np.zeros(test_events)\n",
    "prev = 0\n",
    "for i in range(test_events):\n",
    "    relevant_pred = predictions[prev:prev+min_window]\n",
    "    predictions_majority[i] = np.argmax(np.bincount(relevant_pred))\n",
    "    prev = prev+int(min_window)\n",
    "print('predictions after majority voting:')\n",
    "print(predictions_majority)\n",
    "predictions_majority = predictions_majority.astype(int)\n",
    "real_labels = np.array([0,0,1,1,2,2,3,3,4,4,5,5,6,6,7,7,8,8,9,9]*len(pos_session_list))\n",
    "accuracy = np.mean(predictions_majority==real_labels)\n",
    "print('Real labels:')\n",
    "print(real_labels)\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "print('Test accuracy after majority voting:')\n",
    "print(accuracy)\n",
    "print('Confusion matrix:')\n",
    "plt.imshow(confusion_matrix(real_labels, predictions_majority))\n",
    "plt.colorbar()\n",
    "#plt.show()\n",
    "summary_name = 'summaries/subject_'+subject_num+'/task2/summary_task2_'+subject_num+'_type_SVM_run'+run\n",
    "f = open(summary_name+'.txt', 'w')\n",
    "f.write('Run summary: task 2, subject number '+subject_num+', algorithm type: SVM, run '+run+'\\n')\n",
    "f.write('sessions: ')\n",
    "for s_ind, s in enumerate(train_sessions):\n",
    "  if s_ind != len(train_sessions) - 1:\n",
    "    f.write(s+', ')\n",
    "  else:\n",
    "    f.write(s+'\\n')\n",
    "f.write('positions: ')\n",
    "for p_ind, p in enumerate(train_positions):\n",
    "  if p_ind != len(train_positions) - 1:\n",
    "    f.write(p+', ')\n",
    "  else:\n",
    "    f.write(p+'\\n')   \n",
    "f.write('C: '+str(c)+'\\n')\n",
    "f.write('model name: '+SVM_model_name+'.sav'+'\\n')\n",
    "f.write('single frame accuracy: '+str(single_acc)+'\\n')\n",
    "f.write('accuracy after majority voting: '+str(accuracy)+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "d66A1S_3Cyy8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classification task 3"
   ],
   "metadata": {
    "id": "ulvFa7TuCzJU"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQWBjrGzCzJg"
   },
   "source": [
    "4 classification algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CNN"
   ],
   "metadata": {
    "id": "Ii2CmKR3CzJg"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training the backbone CNN:"
   ],
   "metadata": {
    "id": "G0OCHAFwCzJh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "use_hmm = True\n",
    "run = '3'\n",
    "train_sessions = ['1']\n",
    "subject_nums = ['001', '004', '007']\n",
    "train_positions = ['1', '2', '3']\n",
    "min_window = 11\n",
    "\n",
    "sub_pos_session_list = []\n",
    "for sub in subject_nums:\n",
    "  for s in train_sessions:\n",
    "    for p in train_positions:\n",
    "      sub_pos_session_list.append((sub, p, s))\n",
    "\n",
    "test_events = 20*len(sub_pos_session_list)"
   ],
   "metadata": {
    "id": "Dsei_lvvIo51"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Read the files:\n",
    "train_data = None\n",
    "train_labels = None\n",
    "test_data = None\n",
    "test_labels = None\n",
    "if use_hmm:\n",
    "  name_start = 'hmm_generated_files/subject_'#+subject_num+'/gf_'+subject_num\n",
    "  name_start_val = 'segmented_files/subject_'#+subject_num+'/sf_'+subject_num\n",
    "else:\n",
    "  name_start = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "  name_start_val = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "for filename in [(name_start+subject_num+'/gf_'+subject_num+'_pos'+position+'_s'+session+'_train_data.pkl', name_start+subject_num+'/gf_'+subject_num+'_pos'+position+'_s'+session+'_train_labels.pkl', name_start_val+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'+session+'_val_data.pkl', name_start_val+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'+session+'_val_labels.pkl') for subject_num, position, session in sub_pos_session_list]:\n",
    "#for filename in [(pre+'_generated_train_data_10gest.pkl', pre+'_generated_train_labels_10gest.pkl', pre+'_val_data_10gest.pkl', pre+'_val_labels_10gest.pkl') for pre in ['grc1pos1', 'grc1pos2']]:\n",
    "    if filename[0] is not None:\n",
    "        data = pd.read_pickle(filename[0])\n",
    "        data = data.to_numpy()\n",
    "    else:\n",
    "        data = None\n",
    "    print('train data read!')\n",
    "    if filename[1] is not None:\n",
    "        labels = pd.read_pickle(filename[1])\n",
    "        labels = labels.to_numpy()\n",
    "    else:\n",
    "        labels = None\n",
    "    print('train labels read!')\n",
    "    if filename[2] is not None:\n",
    "        test_data_part = pd.read_pickle(filename[2])\n",
    "        test_data_part = test_data_part.to_numpy()\n",
    "    else:\n",
    "        test_data_part = None\n",
    "    print('test data read!')\n",
    "    if filename[3] is not None:\n",
    "        test_labels_part = pd.read_pickle(filename[3])\n",
    "        test_labels_part = test_labels_part.to_numpy()\n",
    "    else:\n",
    "        test_labels_part = None\n",
    "    print('test labels read!')\n",
    "\n",
    "    if train_data is not None and data is not None:\n",
    "        train_data = np.concatenate((train_data, data))\n",
    "        train_labels = np.concatenate((train_labels, labels))\n",
    "    if test_data is not None and test_data_part is not None:\n",
    "        test_data = np.concatenate((test_data, test_data_part))\n",
    "        test_labels = np.concatenate((test_labels, test_labels_part))\n",
    "    if train_data is None or test_data is None:\n",
    "        if data is not None and train_data is None:\n",
    "            train_data = copy.deepcopy(data)\n",
    "            train_labels = copy.deepcopy(labels)\n",
    "        if test_data_part is not None and test_data is None:\n",
    "            test_data = copy.deepcopy(test_data_part)\n",
    "            test_labels = copy.deepcopy(test_labels_part)\n",
    "\n",
    "train_data = np.reshape(train_data, (train_data.shape[0], 4, 4))\n",
    "test_data = np.reshape(test_data, (test_data.shape[0], 4, 4))\n",
    "print('shapes of train data, train labels, test data, test labels:')\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n",
    "per = np.random.permutation(len(train_data))\n",
    "train_data = train_data[per, :]\n",
    "train_labels = train_labels[per]\n",
    "# Data standartization:\n",
    "train_data = train_data - np.mean(train_data, axis=0)\n",
    "train_data = train_data / np.std(train_data, axis=0)\n",
    "test_data = test_data - np.mean(test_data, axis=0)\n",
    "test_data = test_data / np.std(test_data, axis=0)\n",
    "\n",
    "train_data = tf.expand_dims(train_data, axis=-1)\n",
    "test_data = tf.expand_dims(test_data, axis=-1)\n",
    "\n",
    "# Hyper-parameters:\n",
    "if use_hmm:\n",
    "  algo_type='CNN+HMM'\n",
    "else:\n",
    "  algo_type='CNN'\n",
    "reg_strength = 0#0.0001\n",
    "lr = 1e-2#1e-3#5e-4\n",
    "d1 = 0.2\n",
    "epochs_num = 500#1000#500#1500#2000 #1500\n",
    "batch_size = 1000#3500# #5000\n",
    "#min_window = 11 #17\n",
    "#min_window = real_min_window\n",
    "print('min_window='+str(min_window))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(2, 2, padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_strength)),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(4, 2, padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_strength)),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(40, kernel_regularizer=tf.keras.regularizers.l2(reg_strength)),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(d1),\n",
    "    tf.keras.layers.Dense(20, kernel_regularizer=tf.keras.regularizers.l2(reg_strength)),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(d1),\n",
    "    tf.keras.layers.Dense(10, kernel_regularizer=tf.keras.regularizers.l2(reg_strength), name='new'),\n",
    "    tf.keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "def scheduler(epoch):\n",
    "  if epoch>100:\n",
    "    return lr/2\n",
    "  else:\n",
    "    return lr\n",
    "\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_data, train_labels, epochs=epochs_num,batch_size=batch_size, callbacks=[callback])\n",
    "model.save_weights('weights/CNN/weights_task3_bb_full_sub_run'+run)\n",
    "#model.load_weights('weights/CNN/weights_task3_bb_full_sub_run'+run)\n",
    "test_loss, test_acc = model.evaluate(test_data,  test_labels)\n",
    "print('\\nSingle frame based validation accuracy:', test_acc)\n",
    "predictions = np.argmax(model.predict(test_data), axis=1)\n",
    "print('Network predictions:')\n",
    "print(predictions)\n",
    "print(len(predictions))\n",
    "print('Real labels:')\n",
    "print(test_labels.T)\n",
    "\n",
    "# majority voting:\n",
    "predictions_majority = np.zeros(test_events)\n",
    "prev = 0\n",
    "for i in range(test_events):\n",
    "    # if i < test_events/2:\n",
    "    #   min_window = 11\n",
    "    # else:\n",
    "    #   min_window = 11\n",
    "    relevant_pred = predictions[prev:prev+min_window]\n",
    "    predictions_majority[i] = np.argmax(np.bincount(relevant_pred))\n",
    "    prev = prev+int(min_window)\n",
    "print('Network predictions after majority voting:')\n",
    "print(predictions_majority)\n",
    "predictions_majority = predictions_majority.astype(int)\n",
    "real_labels = np.array([0,0,1,1,2,2,3,3,4,4,5,5,6,6,7,7,8,8,9,9]*len(sub_pos_session_list))\n",
    "accuracy = np.mean(predictions_majority==real_labels)\n",
    "print('Real labels:')\n",
    "print(real_labels)\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].plot(history.history['accuracy'])\n",
    "axs[0].set(title='Training Accuracy', xlabel='epochs', ylabel='accuracy')\n",
    "axs[1].plot(history.history['loss'])\n",
    "axs[1].set(title='Training Loss', xlabel='epochs', ylabel='loss')\n",
    "#model.summary()\n",
    "plt.show()\n",
    "\n",
    "print('Validation accuracy after majority voting:')\n",
    "print(accuracy)\n",
    "print('Confusion matrix:')\n",
    "plt.imshow(confusion_matrix(real_labels, predictions_majority))\n",
    "plt.colorbar()\n",
    "#plt.show()\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ntJ3SFUwHxfg",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1654343030398,
     "user_tz": -180,
     "elapsed": 86404,
     "user": {
      "displayName": "Adi Ben-Ari",
      "userId": "15772198121198769105"
     }
    },
    "outputId": "0c6b3b72-2a94-4fd9-d972-8ee87299daaf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CNN fine tuning:"
   ],
   "metadata": {
    "id": "4jWuD2gUCzJm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "use_hmm = False\n",
    "run = '3'\n",
    "ft_run = '1'\n",
    "ft_train_sessions = ['2']\n",
    "subject_num = '004'\n",
    "ft_train_positions = ['1']\n",
    "\n",
    "ft_pos_session_list = []\n",
    "for s in ft_train_sessions:\n",
    "  for p in ft_train_positions:\n",
    "    ft_pos_session_list.append((p, s))\n",
    "\n",
    "ft_test_events = 20*len(ft_pos_session_list)"
   ],
   "metadata": {
    "id": "LHGIGNHxCzJn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Read the files:\n",
    "train_data = None\n",
    "train_labels = None\n",
    "test_data = None\n",
    "test_labels = None\n",
    "if use_hmm:\n",
    "  name_start = 'hmm_generated_files/subject_'+subject_num+'/gf_'+subject_num\n",
    "  name_start_val = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "else:\n",
    "  name_start = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "  name_start_val = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "for filename in [(name_start+'_pos'+position+'_s'+session+'_test_data.pkl', name_start+'_pos'+position+'_s'+session+'_test_labels.pkl', name_start_val+'_pos'+position+'_s'+session+'_val_data.pkl', name_start_val+'_pos'+position+'_s'+session+'_val_labels.pkl') for position, session in ft_pos_session_list]:\n",
    "#for filename in [(pre+'_generated_train_data_10gest.pkl', pre+'_generated_train_labels_10gest.pkl', pre+'_val_data_10gest.pkl', pre+'_val_labels_10gest.pkl') for pre in ['grc1pos1', 'grc1pos2']]:\n",
    "    if filename[0] is not None:\n",
    "        data = pd.read_pickle(filename[0])\n",
    "        data = data.to_numpy()\n",
    "    else:\n",
    "        data = None\n",
    "    print('train data read!')\n",
    "    if filename[1] is not None:\n",
    "        labels = pd.read_pickle(filename[1])\n",
    "        labels = labels.to_numpy()\n",
    "    else:\n",
    "        labels = None\n",
    "    print('train labels read!')\n",
    "    if filename[2] is not None:\n",
    "        test_data_part = pd.read_pickle(filename[2])\n",
    "        test_data_part = test_data_part.to_numpy()\n",
    "    else:\n",
    "        test_data_part = None\n",
    "    print('test data read!')\n",
    "    if filename[3] is not None:\n",
    "        test_labels_part = pd.read_pickle(filename[3])\n",
    "        test_labels_part = test_labels_part.to_numpy()\n",
    "    else:\n",
    "        test_labels_part = None\n",
    "    print('test labels read!')\n",
    "\n",
    "    if train_data is not None and data is not None:\n",
    "        train_data = np.concatenate((train_data, data))\n",
    "        train_labels = np.concatenate((train_labels, labels))\n",
    "    if test_data is not None and test_data_part is not None:\n",
    "        test_data = np.concatenate((test_data, test_data_part))\n",
    "        test_labels = np.concatenate((test_labels, test_labels_part))\n",
    "    if train_data is None or test_data is None:\n",
    "        if data is not None and train_data is None:\n",
    "            train_data = copy.deepcopy(data)\n",
    "            train_labels = copy.deepcopy(labels)\n",
    "        if test_data_part is not None and test_data is None:\n",
    "            test_data = copy.deepcopy(test_data_part)\n",
    "            test_labels = copy.deepcopy(test_labels_part)\n",
    "\n",
    "# # One shot:\n",
    "# train_mask1 = [True for i in range(min_window)]\n",
    "# train_mask2 = [False for i in range(min_window)]\n",
    "# train_mask = np.array((train_mask1+train_mask2)*10)\n",
    "# train_data = train_data[train_mask,:]\n",
    "# train_labels = train_labels[train_mask]\n",
    "\n",
    "train_data = np.reshape(train_data, (train_data.shape[0], 4, 4))\n",
    "test_data = np.reshape(test_data, (test_data.shape[0], 4, 4))\n",
    "# Add hmm data:\n",
    "d = {}\n",
    "for comp_num in [4]:\n",
    "    for iterations_num in [10]:\n",
    "        res = []\n",
    "        tries_num = 1\n",
    "        gestures_num = 10\n",
    "        repetitions = 2\n",
    "        val_num = 2\n",
    "        lengths = [train_data.shape[1] for j in range(repetitions)]\n",
    "        final_res = []\n",
    "        models = []\n",
    "\n",
    "        for i in range(gestures_num):\n",
    "            X = np.concatenate([train_data[repetitions*i+j,:,:] for j in range(repetitions)])\n",
    "            model = hmm.GaussianHMM(n_components=comp_num, covariance_type=\"diag\", n_iter=iterations_num)\n",
    "            model.fit(X, lengths)\n",
    "            models.append(model)\n",
    "            #folder_path = 'hmm_models/subject_'+subject_num+'/hmm_'+subject_num+'_pos'+position+'_s'+session \n",
    "            #if not os.path.exists(folder_path):\n",
    "            #    os.makedirs(folder_path)\n",
    "            #with open(folder_path+'/hmm_gest'+str(i)+'.pkl', \"wb\") as file: pickle.dump(model, file)\n",
    "        test_res = []\n",
    "        for s in range(gestures_num*val_num):\n",
    "            test_res.append(np.argmax(np.array([models[i].score(test_data[s, :, :]) for i in range(gestures_num)])))\n",
    "\n",
    "        print('test results:')\n",
    "        print(test_res)\n",
    "        print('real labels:')\n",
    "        print(test_labels.T)\n",
    "        acc = np.mean(np.array(test_res)==test_labels)\n",
    "        print('HMM classification accuracy:')\n",
    "        print(np.mean(np.array(test_res)==test_labels))\n",
    "\n",
    "\n",
    "print('shapes of train data, train labels, test data, test labels:')\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n",
    "per = np.random.permutation(len(train_data))\n",
    "train_data = train_data[per, :]\n",
    "train_labels = train_labels[per]\n",
    "# Data standartization:\n",
    "train_data = train_data - np.mean(train_data, axis=0)\n",
    "train_data = train_data / np.std(train_data, axis=0)\n",
    "test_data = test_data - np.mean(test_data, axis=0)\n",
    "test_data = test_data / np.std(test_data, axis=0)\n",
    "\n",
    "train_data = tf.expand_dims(train_data, axis=-1)\n",
    "test_data = tf.expand_dims(test_data, axis=-1)\n",
    "\n",
    "# Hyper-parameters:\n",
    "if use_hmm:\n",
    "  algo_type='CNN+HMM'\n",
    "else:\n",
    "  algo_type='CNN'\n",
    "reg_strength = 0.001\n",
    "lr = 1e-3\n",
    "d1 = 0.3\n",
    "epochs_num = 400\n",
    "batch_size = 150#250\n",
    "#min_window = 11 #17\n",
    "#min_window = real_min_window\n",
    "print('min_window='+str(min_window))\n",
    "\n",
    "layer1 = tf.keras.layers.Conv2D(2, 2, padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_strength))\n",
    "layer1.trainable = False\n",
    "layer2 = tf.keras.layers.BatchNormalization()\n",
    "layer2.trainable = False\n",
    "layer3 =  tf.keras.layers.Conv2D(4, 2, padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_strength))\n",
    "layer3.trainable = False\n",
    "layer4 = tf.keras.layers.BatchNormalization()\n",
    "layer4.trainable = True\n",
    "layer5 = tf.keras.layers.Dense(40, kernel_regularizer=tf.keras.regularizers.l2(reg_strength))\n",
    "layer5.trainable = True\n",
    "layer6 = tf.keras.layers.BatchNormalization()\n",
    "layer6.trainable = True\n",
    "layer7 = tf.keras.layers.Dense(20, kernel_regularizer=tf.keras.regularizers.l2(reg_strength))\n",
    "layer7.trainable = True\n",
    "layer8 = tf.keras.layers.BatchNormalization()\n",
    "layer8.trainable = True\n",
    "layer9 = tf.keras.layers.Dense(10, kernel_regularizer=tf.keras.regularizers.l2(reg_strength), name='new')\n",
    "layer9.trainable = True\n",
    "min_angle = 1/18\n",
    "max_shift_h = 0.05\n",
    "max_shift_w = 0.05\n",
    "model = tf.keras.Sequential([\n",
    "    #tf.keras.layers.RandomTranslation(((-1)*max_shift_h, max_shift_h), ((-1)*max_shift_w, max_shift_w), fill_mode='nearest'),\n",
    "    #tf.keras.layers.RandomRotation(((-1)*min_angle, min_angle)),\n",
    "    layer1,\n",
    "    #tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    #tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=1),\n",
    "    layer2,\n",
    "    layer3,\n",
    "    #tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    layer4,\n",
    "    #tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=1),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    layer5,\n",
    "    tf.keras.layers.ReLU(),\n",
    "    layer6,\n",
    "    ##tf.keras.layers.Dropout(d1),\n",
    "    layer7,\n",
    "    tf.keras.layers.ReLU(),\n",
    "    layer8,\n",
    "    tf.keras.layers.Dropout(d1),\n",
    "    layer9,\n",
    "    tf.keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "def scheduler(epoch):\n",
    "  if epoch < 600:\n",
    "    return lr\n",
    "  else:\n",
    "    return lr\n",
    "\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.load_weights('weights/CNN/weights_task3_bb_full_sub_run'+run)\n",
    "history = model.fit(train_data, train_labels, epochs=epochs_num,batch_size=batch_size, callbacks=[callback])\n",
    "model.save_weights('weights/CNN/weights_task3_ft_'+subject_num+'_pos'+ft_train_positions[0]+'_bb_run'+run+'_ft_run'+ft_run)\n",
    "test_loss, test_acc = model.evaluate(test_data,  test_labels)\n",
    "print('\\nSingle frame based validation accuracy:', test_acc)\n",
    "predictions = np.argmax(model.predict(test_data), axis=1)\n",
    "print('Network predictions:')\n",
    "print(predictions)\n",
    "print('Real labels:')\n",
    "print(test_labels.T)\n",
    "\n",
    "# majority voting:\n",
    "predictions_majority = np.zeros(ft_test_events)\n",
    "prev = 0\n",
    "for i in range(ft_test_events):\n",
    "    relevant_pred = predictions[prev:prev+min_window]\n",
    "    predictions_majority[i] = np.argmax(np.bincount(relevant_pred))\n",
    "    prev = prev+int(min_window)\n",
    "print('Network predictions after majority voting:')\n",
    "print(predictions_majority)\n",
    "predictions_majority = predictions_majority.astype(int)\n",
    "real_labels = np.array([0,0,1,1,2,2,3,3,4,4,5,5,6,6,7,7,8,8,9,9]*len(ft_pos_session_list))\n",
    "accuracy = np.mean(predictions_majority==real_labels)\n",
    "print('Real labels:')\n",
    "print(real_labels)\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].plot(history.history['accuracy'])\n",
    "axs[0].set(title='Training Accuracy', xlabel='epochs', ylabel='accuracy')\n",
    "axs[1].plot(history.history['loss'])\n",
    "axs[1].set(title='Training Loss', xlabel='epochs', ylabel='loss')\n",
    "#model.summary()\n",
    "plt.show()\n",
    "\n",
    "print('Validation accuracy after majority voting:')\n",
    "print(accuracy)\n",
    "print('Confusion matrix:')\n",
    "plt.imshow(confusion_matrix(real_labels, predictions_majority))\n",
    "plt.colorbar()\n",
    "#plt.show()\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1654350790827,
     "user_tz": -180,
     "elapsed": 18506,
     "user": {
      "displayName": "Adi Ben-Ari",
      "userId": "15772198121198769105"
     }
    },
    "outputId": "751dc807-848b-4271-8e61-e8a87cf9c87a",
    "id": "uugYdzvkCzJn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "test fine tuned CNN"
   ],
   "metadata": {
    "id": "CpZ5NHnFlBvx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Read the files:\n",
    "train_data = None\n",
    "train_labels = None\n",
    "test_data = None\n",
    "test_labels = None\n",
    "name_start = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "name_start_val = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "for filename in [(name_start_val+'_pos'+position+'_s'+session+'_train_data.pkl', name_start_val+'_pos'+position+'_s'+session+'_train_labels.pkl') for position, session in ft_pos_session_list]:    \n",
    "    if filename[0] is not None:\n",
    "        test_data_part = pd.read_pickle(filename[0])\n",
    "        test_data_part = test_data_part.to_numpy()\n",
    "    else:\n",
    "        test_data_part = None\n",
    "    print('test data read!')\n",
    "    if filename[1] is not None:\n",
    "        test_labels_part = pd.read_pickle(filename[1])\n",
    "        test_labels_part = test_labels_part.to_numpy()\n",
    "    else:\n",
    "        test_labels_part = None\n",
    "    print('test labels read!')\n",
    "\n",
    "    if train_data is not None and data is not None:\n",
    "        train_data = np.concatenate((train_data, data))\n",
    "        train_labels = np.concatenate((train_labels, labels))\n",
    "    if test_data is not None and test_data_part is not None:\n",
    "        test_data = np.concatenate((test_data, test_data_part))\n",
    "        test_labels = np.concatenate((test_labels, test_labels_part))\n",
    "    if train_data is None or test_data is None:\n",
    "        if data is not None and train_data is None:\n",
    "            train_data = copy.deepcopy(data)\n",
    "            train_labels = copy.deepcopy(labels)\n",
    "        if test_data_part is not None and test_data is None:\n",
    "            test_data = copy.deepcopy(test_data_part)\n",
    "            test_labels = copy.deepcopy(test_labels_part)\n",
    "\n",
    "test_data = np.reshape(test_data, (test_data.shape[0], 4, 4))\n",
    "print('shapes of test data, test labels:')\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "# Data standartization:\n",
    "test_data = test_data - np.mean(test_data, axis=0)\n",
    "test_data = test_data / np.std(test_data, axis=0)\n",
    "\n",
    "test_data = tf.expand_dims(test_data, axis=-1)\n",
    "\n",
    "model.load_weights('weights/CNN/weights_task3_ft_'+subject_num+'_pos'+ft_train_positions[0]+'_bb_run'+run+'_ft_run'+ft_run)\n",
    "test_loss, test_acc = model.evaluate(test_data,  test_labels)\n",
    "print('\\nSingle frame based test accuracy:', test_acc)\n",
    "predictions = np.argmax(model.predict(test_data), axis=1)\n",
    "print('Network predictions:')\n",
    "print(predictions)\n",
    "print('Real labels:')\n",
    "print(test_labels.T)\n",
    "\n",
    "# majority voting:\n",
    "predictions_majority = np.zeros(60*len(ft_pos_session_list))\n",
    "prev = 0\n",
    "for i in range(60*len(ft_pos_session_list)):\n",
    "    relevant_pred = predictions[prev:prev+min_window]\n",
    "    predictions_majority[i] = np.argmax(np.bincount(relevant_pred))\n",
    "    prev = prev+int(min_window)\n",
    "print('Network predictions after majority voting:')\n",
    "print(predictions_majority)\n",
    "predictions_majority = predictions_majority.astype(int)\n",
    "real_labels = np.array([0,0,0,0,0,0,1,1,1,1,1,1,2,2,2,2,2,2,3,3,3,3,3,3,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,6,6,6,7,7,7,7,7,7,8,8,8,8,8,8,9,9,9,9,9,9]*len(ft_pos_session_list))\n",
    "accuracy = np.mean(predictions_majority==real_labels)\n",
    "print('Real labels:')\n",
    "print(real_labels)\n",
    "print(len(real_labels))\n",
    "\n",
    "print('Test accuracy after majority voting:')\n",
    "print(accuracy)\n",
    "print('Confusion matrix:')\n",
    "mat = confusion_matrix(real_labels, predictions_majority)\n",
    "print(repr(mat))\n",
    "plt.imshow(mat)\n",
    "#plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "folder_path = 'summaries/subject_'+subject_num+'/task3'\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "summary_name = 'summaries/subject_'+subject_num+'/task3/summary_task3_ft_'+subject_num+'_pos'+ft_train_positions[0]+'_type_'+algo_type+'_bb_run'+run+'_ft_run'+ft_run\n",
    "f = open(summary_name+'.txt', 'w')\n",
    "f.write('Run summary: task 3 ft, subject number '+subject_num+', algorithm type: '+algo_type+',backbone run '+run+', fine tuning run'+ft_run+'\\n')\n",
    "f.write('sessions: ')\n",
    "for s_ind, s in enumerate(train_sessions):\n",
    "  if s_ind != len(train_sessions) - 1:\n",
    "    f.write(s+', ')\n",
    "  else:\n",
    "    f.write(s+'\\n')\n",
    "f.write('positions: ')\n",
    "for p_ind, p in enumerate(train_positions):\n",
    "  if p_ind != len(train_positions) - 1:\n",
    "    f.write(p+', ')\n",
    "  else:\n",
    "    f.write(p+'\\n')   \n",
    "f.write('regularization strength: '+str(reg_strength)+'\\n')\n",
    "f.write('learning rate: '+str(lr)+'\\n')\n",
    "f.write('dropout: '+str(d1)+'\\n')\n",
    "f.write('number of epochs: '+str(epochs_num)+'\\n')\n",
    "f.write('batch size: '+str(batch_size)+'\\n')\n",
    "f.write('minimal window length (in samples): '+str(min_window)+'\\n')\n",
    "f.write('Single frame based validation accuracy: '+str(test_acc)+'\\n')\n",
    "f.write('Test accuracy after majority voting: '+str(accuracy)+'\\n')\n",
    "f.write('weights file: weights/CNN/weights_task3_ft_'+subject_num+'_bb_run'+run+'_ft_run'+ft_run)\n",
    "f.close()\n",
    "\n",
    "confusion_matrix_df = pd.DataFrame(data=mat)\n",
    "confusion_matrix_df.to_pickle(folder_path+'/CNN_ft_mat_bb_run'+run+'_ft_run'+ft_run+'subject_'+subject_num+'_pos'+ft_train_positions[0]+'.pkl', protocol=4)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "OKaAiqEwlGPQ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1654354252111,
     "user_tz": -180,
     "elapsed": 1231,
     "user": {
      "displayName": "Adi Ben-Ari",
      "userId": "15772198121198769105"
     }
    },
    "outputId": "8da6d0c9-f8d2-4796-c3e7-03274904d356"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RNN"
   ],
   "metadata": {
    "id": "JBg_cD62CzJt"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training the backbone RNN"
   ],
   "metadata": {
    "id": "KSq6lQsrCzJt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "use_hmm = True\n",
    "run = '1'"
   ],
   "metadata": {
    "id": "SVZQdfJ-CzJu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Read the files:\n",
    "train_data = None\n",
    "train_labels = None\n",
    "test_data = None\n",
    "test_labels = None\n",
    "# if use_hmm:\n",
    "#   name_start = 'hmm_generated_files/subject_'+subject_num+'/gf_'+subject_num\n",
    "#   name_start_val = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "# else:\n",
    "#   name_start = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "#   name_start_val = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "# for filename in [(name_start+'_pos'+position+'_s'+session+'_train_data.pkl', name_start+'_pos'+position+'_s'+session+'_train_labels.pkl', name_start_val+'_pos'+position+'_s'+session+'_val_data.pkl', name_start_val+'_pos'+position+'_s'+session+'_val_labels.pkl') for position, session in pos_session_list]:\n",
    "if use_hmm:\n",
    "  name_start = 'hmm_generated_files/subject_'#+subject_num+'/gf_'+subject_num\n",
    "  name_start_val = 'segmented_files/subject_'#+subject_num+'/sf_'+subject_num\n",
    "else:\n",
    "  name_start = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "  name_start_val = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "for filename in [(name_start+subject_num+'/gf_'+subject_num+'_pos'+position+'_s'+session+'_train_data.pkl', name_start+subject_num+'/gf_'+subject_num+'_pos'+position+'_s'+session+'_train_labels.pkl', name_start_val+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'+session+'_val_data.pkl', name_start_val+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'+session+'_val_labels.pkl') for subject_num, position, session in sub_pos_session_list]:\n",
    "#for filename in [(pre+'_generated_train_data_10gest.pkl', pre+'_generated_train_labels_10gest.pkl', pre+'_val_data_10gest.pkl', pre+'_val_labels_10gest.pkl') for pre in ['grc1pos1', 'grc1pos2']]:\n",
    "    if filename[0] is not None:\n",
    "        data = pd.read_pickle(filename[0])\n",
    "        data = data.to_numpy()\n",
    "    else:\n",
    "        data = None\n",
    "    print('train data read!')\n",
    "    if filename[1] is not None:\n",
    "        labels = pd.read_pickle(filename[1])\n",
    "        labels = labels.to_numpy()\n",
    "    else:\n",
    "        labels = None\n",
    "    print('train labels read!')\n",
    "    if filename[2] is not None:\n",
    "        test_data_part = pd.read_pickle(filename[2])\n",
    "        test_data_part = test_data_part.to_numpy()\n",
    "    else:\n",
    "        test_data_part = None\n",
    "    print('test data read!')\n",
    "    if filename[3] is not None:\n",
    "        test_labels_part = pd.read_pickle(filename[3])\n",
    "        test_labels_part = test_labels_part.to_numpy()\n",
    "    else:\n",
    "        test_labels_part = None\n",
    "    print('test labels read!')\n",
    "\n",
    "    if train_data is not None and data is not None:\n",
    "        train_data = np.concatenate((train_data, data))\n",
    "        train_labels = np.concatenate((train_labels, labels))\n",
    "    if test_data is not None and test_data_part is not None:\n",
    "        test_data = np.concatenate((test_data, test_data_part))\n",
    "        test_labels = np.concatenate((test_labels, test_labels_part))\n",
    "    if train_data is None or test_data is None:\n",
    "        if data is not None and train_data is None:\n",
    "            train_data = copy.deepcopy(data)\n",
    "            train_labels = copy.deepcopy(labels)\n",
    "        if test_data_part is not None and test_data is None:\n",
    "            test_data = copy.deepcopy(test_data_part)\n",
    "            test_labels = copy.deepcopy(test_labels_part)\n",
    "\n",
    "\n",
    "# Hyper-parameters:\n",
    "if use_hmm:\n",
    "  algo_type='RNN+HMM'\n",
    "else:\n",
    "  algo_type='RNN'\n",
    "reg_strength = 0.0001\n",
    "lr = 1e-2\n",
    "d1 = 0.1\n",
    "#min_angle = 1/18\n",
    "#max_shift_h = 0.05\n",
    "#max_shift_w = 0.05\n",
    "epochs_num = 1000 #1500\n",
    "batch_size = 800#400\n",
    "#min_window = 11 #17\n",
    "print('min_window='+str(min_window))\n",
    "\n",
    "windows_num = min_window\n",
    "reshaped_train_data = np.zeros((int(train_data.shape[0]/windows_num), windows_num, 16))\n",
    "reshaped_test_data = np.zeros((int(test_data.shape[0]/windows_num), windows_num, 16))\n",
    "for i in range(reshaped_train_data.shape[0]):\n",
    "    reshaped_train_data[i,:,:] = train_data[windows_num*i:windows_num*(i+1)]\n",
    "for i in range(reshaped_test_data.shape[0]):\n",
    "    reshaped_test_data[i,:,:] = test_data[windows_num*i:windows_num*(i+1)]\n",
    "train_data = reshaped_train_data\n",
    "test_data = reshaped_test_data\n",
    "train_labels = train_labels[::windows_num]\n",
    "test_labels = test_labels[::windows_num]\n",
    "print('shapes:')\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "\n",
    "per = np.random.permutation(len(train_data))\n",
    "train_data = train_data[per, :]\n",
    "train_labels = train_labels[per]\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(12, dropout=d1, input_shape=(windows_num,16), name='lstm2'),\n",
    "    tf.keras.layers.Dense(10, kernel_regularizer=tf.keras.regularizers.l2(reg_strength), name='new'),\n",
    "    tf.keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "def scheduler(epoch):\n",
    "  if epoch < 600:\n",
    "    return lr\n",
    "  else:\n",
    "    return lr\n",
    "\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_data, train_labels, epochs=epochs_num,batch_size=batch_size, callbacks=[callback])\n",
    "model.save_weights('weights/RNN/weights_task3_bb_full_sub_run'+run)\n",
    "#model.load_weights('weights/RNN/weights_task2_'+subject_num+'_pos'+position+'_run'+run)\n",
    "test_loss, test_acc = model.evaluate(test_data,  test_labels)#, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "\n",
    "predictions = np.argmax(model.predict(test_data), axis=1)\n",
    "cmat = confusion_matrix(test_labels, predictions)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].plot(history.history['accuracy'])\n",
    "axs[0].set(title='Training Accuracy', xlabel='epochs', ylabel='accuracy')\n",
    "axs[1].plot(history.history['loss'])\n",
    "axs[1].set(title='Training Loss', xlabel='epochs', ylabel='loss')\n",
    "#model.summary()\n",
    "plt.figure()\n",
    "print('Confusion matrix:')\n",
    "plt.imshow(cmat)\n",
    "plt.colorbar()\n",
    "#plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1654343623786,
     "user_tz": -180,
     "elapsed": 42074,
     "user": {
      "displayName": "Adi Ben-Ari",
      "userId": "15772198121198769105"
     }
    },
    "outputId": "91764252-ea9c-4c60-d78c-fdf384b127d7",
    "id": "NAN1zJdOCzJu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### fine tune RNN:"
   ],
   "metadata": {
    "id": "e3Z_Kcj7jC5G"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "use_hmm = False\n",
    "ft_run = '1'"
   ],
   "metadata": {
    "id": "QTJxYyJ3kgwW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Read the files:\n",
    "train_data = None\n",
    "train_labels = None\n",
    "test_data = None\n",
    "test_labels = None\n",
    "if use_hmm:\n",
    "  name_start = 'hmm_generated_files/subject_'+subject_num+'/gf_'+subject_num\n",
    "  name_start_val = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "else:\n",
    "  name_start = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "  name_start_val = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "for filename in [(name_start+'_pos'+position+'_s'+session+'_test_data.pkl', name_start+'_pos'+position+'_s'+session+'_test_labels.pkl', name_start_val+'_pos'+position+'_s'+session+'_val_data.pkl', name_start_val+'_pos'+position+'_s'+session+'_val_labels.pkl') for position, session in ft_pos_session_list]:\n",
    "#for filename in [(pre+'_generated_train_data_10gest.pkl', pre+'_generated_train_labels_10gest.pkl', pre+'_val_data_10gest.pkl', pre+'_val_labels_10gest.pkl') for pre in ['grc1pos1', 'grc1pos2']]:\n",
    "    if filename[0] is not None:\n",
    "        data = pd.read_pickle(filename[0])\n",
    "        data = data.to_numpy()\n",
    "    else:\n",
    "        data = None\n",
    "    print('train data read!')\n",
    "    if filename[1] is not None:\n",
    "        labels = pd.read_pickle(filename[1])\n",
    "        labels = labels.to_numpy()\n",
    "    else:\n",
    "        labels = None\n",
    "    print('train labels read!')\n",
    "    if filename[2] is not None:\n",
    "        test_data_part = pd.read_pickle(filename[2])\n",
    "        test_data_part = test_data_part.to_numpy()\n",
    "    else:\n",
    "        test_data_part = None\n",
    "    print('test data read!')\n",
    "    if filename[3] is not None:\n",
    "        test_labels_part = pd.read_pickle(filename[3])\n",
    "        test_labels_part = test_labels_part.to_numpy()\n",
    "    else:\n",
    "        test_labels_part = None\n",
    "    print('test labels read!')\n",
    "\n",
    "    if train_data is not None and data is not None:\n",
    "        train_data = np.concatenate((train_data, data))\n",
    "        train_labels = np.concatenate((train_labels, labels))\n",
    "    if test_data is not None and test_data_part is not None:\n",
    "        test_data = np.concatenate((test_data, test_data_part))\n",
    "        test_labels = np.concatenate((test_labels, test_labels_part))\n",
    "    if train_data is None or test_data is None:\n",
    "        if data is not None and train_data is None:\n",
    "            train_data = copy.deepcopy(data)\n",
    "            train_labels = copy.deepcopy(labels)\n",
    "        if test_data_part is not None and test_data is None:\n",
    "            test_data = copy.deepcopy(test_data_part)\n",
    "            test_labels = copy.deepcopy(test_labels_part)\n",
    "\n",
    "\n",
    "# Hyper-parameters:\n",
    "if use_hmm:\n",
    "  algo_type='RNN+HMM'\n",
    "else:\n",
    "  algo_type='RNN'\n",
    "reg_strength = 0.0001\n",
    "lr = 1e-2\n",
    "d1 = 0.1\n",
    "#min_angle = 1/18\n",
    "#max_shift_h = 0.05\n",
    "#max_shift_w = 0.05\n",
    "epochs_num = 2000 #1500\n",
    "batch_size = 20 #full\n",
    "#min_window = 11 #17\n",
    "print('min_window='+str(min_window))\n",
    "\n",
    "windows_num = min_window\n",
    "reshaped_train_data = np.zeros((int(train_data.shape[0]/windows_num), windows_num, 16))\n",
    "reshaped_test_data = np.zeros((int(test_data.shape[0]/windows_num), windows_num, 16))\n",
    "for i in range(reshaped_train_data.shape[0]):\n",
    "    reshaped_train_data[i,:,:] = train_data[windows_num*i:windows_num*(i+1)]\n",
    "for i in range(reshaped_test_data.shape[0]):\n",
    "    reshaped_test_data[i,:,:] = test_data[windows_num*i:windows_num*(i+1)]\n",
    "train_data = reshaped_train_data\n",
    "test_data = reshaped_test_data\n",
    "train_labels = train_labels[::windows_num]\n",
    "test_labels = test_labels[::windows_num]\n",
    "print('shapes:')\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "\n",
    "per = np.random.permutation(len(train_data))\n",
    "train_data = train_data[per, :]\n",
    "train_labels = train_labels[per]\n",
    "\n",
    "\n",
    "layer1 = tf.keras.layers.LSTM(12, dropout=d1, input_shape=(windows_num,16), name='lstm2')\n",
    "layer1.trainable = False\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layer1,\n",
    "    tf.keras.layers.Dense(10, kernel_regularizer=tf.keras.regularizers.l2(reg_strength), name='new'),\n",
    "    tf.keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "def scheduler(epoch):\n",
    "  if epoch < 600:\n",
    "    return lr\n",
    "  else:\n",
    "    return lr\n",
    "\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.load_weights('weights/RNN/weights_task3_bb_full_sub_run'+run)\n",
    "history = model.fit(train_data, train_labels, epochs=epochs_num,batch_size=batch_size, callbacks=[callback])\n",
    "model.save_weights('weights/RNN/weights_task3_ft_'+subject_num+'_pos'+ft_train_positions[0]+'_bb_run'+run+'_ft_run'+ft_run)\n",
    "#model.load_weights('weights/RNN/weights_task2_'+subject_num+'_pos'+position+'_run'+run)\n",
    "test_loss, test_acc = model.evaluate(test_data,  test_labels)#, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "\n",
    "predictions = np.argmax(model.predict(test_data), axis=1)\n",
    "cmat = confusion_matrix(test_labels, predictions)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].plot(history.history['accuracy'])\n",
    "axs[0].set(title='Training Accuracy', xlabel='epochs', ylabel='accuracy')\n",
    "axs[1].plot(history.history['loss'])\n",
    "axs[1].set(title='Training Loss', xlabel='epochs', ylabel='loss')\n",
    "#model.summary()\n",
    "plt.figure()\n",
    "print('Confusion matrix:')\n",
    "plt.imshow(cmat)\n",
    "plt.colorbar()\n",
    "#plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1654351345867,
     "user_tz": -180,
     "elapsed": 78699,
     "user": {
      "displayName": "Adi Ben-Ari",
      "userId": "15772198121198769105"
     }
    },
    "outputId": "e02a7f14-e541-4dff-a6cd-640ef4a62f1e",
    "id": "YAERXBvHi9oK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Read the files:\n",
    "train_data = None\n",
    "train_labels = None\n",
    "test_data = None\n",
    "test_labels = None\n",
    "\n",
    "name_start = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "name_start_val = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "for filename in [(name_start+'_pos'+position+'_s'+session+'_train_data.pkl', name_start+'_pos'+position+'_s'+session+'_train_labels.pkl', name_start_val+'_pos'+position+'_s'+session+'_train_data.pkl', name_start_val+'_pos'+position+'_s'+session+'_train_labels.pkl') for position, session in ft_pos_session_list]:\n",
    "#for filename in [(pre+'_generated_train_data_10gest.pkl', pre+'_generated_train_labels_10gest.pkl', pre+'_val_data_10gest.pkl', pre+'_val_labels_10gest.pkl') for pre in ['grc1pos1', 'grc1pos2']]:\n",
    "    if filename[0] is not None:\n",
    "        data = pd.read_pickle(filename[0])\n",
    "        data = data.to_numpy()\n",
    "    else:\n",
    "        data = None\n",
    "    print('train data read!')\n",
    "    if filename[1] is not None:\n",
    "        labels = pd.read_pickle(filename[1])\n",
    "        labels = labels.to_numpy()\n",
    "    else:\n",
    "        labels = None\n",
    "    print('train labels read!')\n",
    "    if filename[2] is not None:\n",
    "        test_data_part = pd.read_pickle(filename[2])\n",
    "        test_data_part = test_data_part.to_numpy()\n",
    "    else:\n",
    "        test_data_part = None\n",
    "    print('test data read!')\n",
    "    if filename[3] is not None:\n",
    "        test_labels_part = pd.read_pickle(filename[3])\n",
    "        test_labels_part = test_labels_part.to_numpy()\n",
    "    else:\n",
    "        test_labels_part = None\n",
    "    print('test labels read!')\n",
    "\n",
    "    if train_data is not None and data is not None:\n",
    "        train_data = np.concatenate((train_data, data))\n",
    "        train_labels = np.concatenate((train_labels, labels))\n",
    "    if test_data is not None and test_data_part is not None:\n",
    "        test_data = np.concatenate((test_data, test_data_part))\n",
    "        test_labels = np.concatenate((test_labels, test_labels_part))\n",
    "    if train_data is None or test_data is None:\n",
    "        if data is not None and train_data is None:\n",
    "            train_data = copy.deepcopy(data)\n",
    "            train_labels = copy.deepcopy(labels)\n",
    "        if test_data_part is not None and test_data is None:\n",
    "            test_data = copy.deepcopy(test_data_part)\n",
    "            test_labels = copy.deepcopy(test_labels_part)\n",
    "\n",
    "\n",
    "# Hyper-parameters:\n",
    "if use_hmm:\n",
    "  algo_type='RNN+HMM'\n",
    "else:\n",
    "  algo_type='RNN'\n",
    "reg_strength = 0.0001\n",
    "lr = 1e-2\n",
    "d1 = 0.1\n",
    "#min_angle = 1/18\n",
    "#max_shift_h = 0.05\n",
    "#max_shift_w = 0.05\n",
    "epochs_num = 2000 #1500\n",
    "batch_size = 20 #full\n",
    "#min_window = 11 #17\n",
    "print('min_window='+str(min_window))\n",
    "\n",
    "\n",
    "windows_num = min_window\n",
    "reshaped_train_data = np.zeros((int(train_data.shape[0]/windows_num), windows_num, 16))\n",
    "reshaped_test_data = np.zeros((int(test_data.shape[0]/windows_num), windows_num, 16))\n",
    "for i in range(reshaped_train_data.shape[0]):\n",
    "    reshaped_train_data[i,:,:] = train_data[windows_num*i:windows_num*(i+1)]\n",
    "for i in range(reshaped_test_data.shape[0]):\n",
    "    reshaped_test_data[i,:,:] = test_data[windows_num*i:windows_num*(i+1)]\n",
    "train_data = reshaped_train_data\n",
    "test_data = reshaped_test_data\n",
    "train_labels = train_labels[::windows_num]\n",
    "test_labels = test_labels[::windows_num]\n",
    "print('shapes:')\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "\n",
    "per = np.random.permutation(len(train_data))\n",
    "train_data = train_data[per, :]\n",
    "train_labels = train_labels[per]\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(12, dropout=d1, input_shape=(windows_num,16), name='lstm2'),\n",
    "    tf.keras.layers.Dense(10, kernel_regularizer=tf.keras.regularizers.l2(reg_strength), name='new'),\n",
    "    tf.keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "def scheduler(epoch):\n",
    "  if epoch < 600:\n",
    "    return lr\n",
    "  else:\n",
    "    return lr\n",
    "\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#history = model.fit(train_data, train_labels, epochs=epochs_num,batch_size=batch_size, callbacks=[callback])\n",
    "#model.save_weights('weights/RNN/weights_task1_'+subject_num+'_pos'+position+'_run'+run)\n",
    "model.load_weights('weights/RNN/weights_task3_ft_'+subject_num+'_pos'+ft_train_positions[0]+'_bb_run'+run+'_ft_run'+ft_run)\n",
    "test_loss, test_acc = model.evaluate(test_data,  test_labels)#, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "\n",
    "predictions = np.argmax(model.predict(test_data), axis=1)\n",
    "cmat = confusion_matrix(test_labels, predictions)\n",
    "\n",
    "folder_path = 'summaries/subject_'+subject_num+'/task3'\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "summary_name = 'summaries/subject_'+subject_num+'/task3/summary_task3_ft_'+subject_num+'_type_'+algo_type+'_bb_run'+run+'_ft_run'+ft_run\n",
    "f = open(summary_name+'.txt', 'w')\n",
    "f.write('Run summary: task 3 ft, subject number '+subject_num+', algorithm type: '+algo_type+',backbone run '+run+', fine tuning run'+ft_run+'\\n')\n",
    "f.write('sessions: ')\n",
    "for s_ind, s in enumerate(train_sessions):\n",
    "  if s_ind != len(train_sessions) - 1:\n",
    "    f.write(s+', ')\n",
    "  else:\n",
    "    f.write(s+'\\n')\n",
    "f.write('positions: ')\n",
    "for p_ind, p in enumerate(train_positions):\n",
    "  if p_ind != len(train_positions) - 1:\n",
    "    f.write(p+', ')\n",
    "  else:\n",
    "    f.write(p+'\\n')   \n",
    "f.write('regularization strength: '+str(reg_strength)+'\\n')\n",
    "f.write('learning rate: '+str(lr)+'\\n')\n",
    "f.write('dropout: '+str(d1)+'\\n')\n",
    "f.write('number of epochs: '+str(epochs_num)+'\\n')\n",
    "f.write('batch size: '+str(batch_size)+'\\n')\n",
    "f.write('minimal window length (in samples): '+str(min_window)+'\\n')\n",
    "f.write('Test accuracy after majority voting: '+str(test_acc)+'\\n')\n",
    "f.write('weights file: weights/RNN/weights_task3_ft_'+subject_num+'_bb_run'+run+'_ft_run'+ft_run)\n",
    "f.close()\n",
    "\n",
    "confusion_matrix_df = pd.DataFrame(data=cmat)\n",
    "confusion_matrix_df.to_pickle(folder_path+'/RNN_mat.pkl', protocol=4)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1654351806498,
     "user_tz": -180,
     "elapsed": 2965,
     "user": {
      "displayName": "Adi Ben-Ari",
      "userId": "15772198121198769105"
     }
    },
    "outputId": "a8e4ce25-a274-4769-ac57-37b7d264c398",
    "id": "MmzP2JVMi9oM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## KNN"
   ],
   "metadata": {
    "id": "zaBdyMrFCzJz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Task 1** - KNN"
   ],
   "metadata": {
    "id": "hcUfe9W1CzJ0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "run = '1'"
   ],
   "metadata": {
    "id": "ecdCXrnYCzJ0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "k_KNN = 1\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Read the files:\n",
    "train_data = None\n",
    "train_labels = None\n",
    "test_data = None\n",
    "test_labels = None\n",
    "\n",
    "name_start_old = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "name_start_val_old = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "name_start = 'segmented_files/subject_'#+subject_num+'/sf_'+subject_num\n",
    "name_start_val = 'segmented_files/subject_'#+subject_num+'/sf_'+subject_num\n",
    "#for filename in [(name_start+'_pos'+position+'_s'+session+'_train_data.pkl', name_start+'_pos'+position+'_s'+session+'_train_labels.pkl', None, None) for position, session in pos_session_list] + [(name_start+'_pos'+position+'_s'+session+'_test_data.pkl', name_start+'_pos'+position+'_s'+session+'_test_labels.pkl', name_start_val+'_pos'+position+'_s'+session+'_train_data.pkl', name_start_val+'_pos'+position+'_s'+session+'_train_labels.pkl') for position, session in ft_pos_session_list]:\n",
    "for filename in [(name_start+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'+session+'_train_data.pkl', name_start+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'+session+'_train_labels.pkl', None, None) for subject_num, position, session in sub_pos_session_list] + [(name_start_old+'_pos'+position+'_s'+session+'_test_data.pkl', name_start_old+'_pos'+position+'_s'+session+'_test_labels.pkl', name_start_val_old+'_pos'+position+'_s'+session+'_train_data.pkl', name_start_val_old+'_pos'+position+'_s'+session+'_train_labels.pkl') for position, session in ft_pos_session_list]:\n",
    "#for filename in [(name_start+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'+session+'_train_data.pkl', name_start+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'+session+'_train_labels.pkl', name_start_val+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'+session+'_val_data.pkl', name_start_val+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'+session+'_val_labels.pkl') for subject_num, position, session in sub_pos_session_list]:\n",
    "#for filename in [(pre+'_generated_train_data_10gest.pkl', pre+'_generated_train_labels_10gest.pkl', pre+'_val_data_10gest.pkl', pre+'_val_labels_10gest.pkl') for pre in ['grc1pos1', 'grc1pos2']]:\n",
    "    if filename[0] is not None:\n",
    "        data = pd.read_pickle(filename[0])\n",
    "        data = data.to_numpy()\n",
    "    else:\n",
    "        data = None\n",
    "    print('train data read!')\n",
    "    if filename[1] is not None:\n",
    "        labels = pd.read_pickle(filename[1])\n",
    "        labels = labels.to_numpy()\n",
    "    else:\n",
    "        labels = None\n",
    "    print('train labels read!')\n",
    "    if filename[2] is not None:\n",
    "        test_data_part = pd.read_pickle(filename[2])\n",
    "        test_data_part = test_data_part.to_numpy()\n",
    "    else:\n",
    "        test_data_part = None\n",
    "    print('test data read!')\n",
    "    if filename[3] is not None:\n",
    "        test_labels_part = pd.read_pickle(filename[3])\n",
    "        test_labels_part = test_labels_part.to_numpy()\n",
    "    else:\n",
    "        test_labels_part = None\n",
    "    print('test labels read!')\n",
    "    # if filename[3] is not None:\n",
    "    #   train_mask1 = [True for i in range(min_window)]\n",
    "    #   train_mask2 = [False for i in range(min_window)]\n",
    "    #   train_mask = np.array((train_mask1+train_mask2)*10)\n",
    "    #   data = data[train_mask,:]\n",
    "    #   labels = labels[train_mask]\n",
    "      #data = None\n",
    "      #labels = None\n",
    "\n",
    "    if train_data is not None and data is not None:\n",
    "        train_data = np.concatenate((train_data, data))\n",
    "        train_labels = np.concatenate((train_labels, labels))\n",
    "    if test_data is not None and test_data_part is not None:\n",
    "        test_data = np.concatenate((test_data, test_data_part))\n",
    "        test_labels = np.concatenate((test_labels, test_labels_part))\n",
    "    if train_data is None or test_data is None:\n",
    "        if data is not None and train_data is None:\n",
    "            train_data = copy.deepcopy(data)\n",
    "            train_labels = copy.deepcopy(labels)\n",
    "        if test_data_part is not None and test_data is None:\n",
    "            test_data = copy.deepcopy(test_data_part)\n",
    "            test_labels = copy.deepcopy(test_labels_part)\n",
    "\n",
    "\n",
    "#test_mask = np.array((train_mask1*2+train_mask2*6)*4)\n",
    "##test_data = test_data[test_mask,:,:]\n",
    "##test_labels = test_labels[test_mask]\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "KNN = KNeighborsClassifier(n_neighbors=k_KNN)\n",
    "KNN.fit(train_data, train_labels)\n",
    "predictions = KNN.predict(test_data).astype('int')\n",
    "single_acc = np.mean(predictions==test_labels.T)\n",
    "print('KNN single frame accuracy: '+str(single_acc))\n",
    "\n",
    "# majority voting:\n",
    "predictions_majority = np.zeros(60*len(ft_pos_session_list))\n",
    "prev = 0\n",
    "for i in range(60*len(ft_pos_session_list)):\n",
    "    relevant_pred = predictions[prev:prev+min_window]\n",
    "    predictions_majority[i] = np.argmax(np.bincount(relevant_pred))\n",
    "    prev = prev+int(min_window)\n",
    "print('predictions after majority voting:')\n",
    "print(predictions_majority)\n",
    "predictions_majority = predictions_majority.astype(int)\n",
    "real_labels = np.array([0,0,0,0,0,0,1,1,1,1,1,1,2,2,2,2,2,2,3,3,3,3,3,3,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,6,6,6,7,7,7,7,7,7,8,8,8,8,8,8,9,9,9,9,9,9]*len(ft_pos_session_list))\n",
    "accuracy = np.mean(predictions_majority==real_labels)\n",
    "print('Real labels:')\n",
    "print(real_labels)\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "print('Test accuracy after majority voting:')\n",
    "print(accuracy)\n",
    "print('Confusion matrix:')\n",
    "plt.imshow(confusion_matrix(real_labels, predictions_majority))\n",
    "plt.colorbar()\n",
    "#plt.show()\n",
    "summary_name = 'summaries/subject_'+subject_num+'/task3/summary_task3_'+subject_num+'_type_KNN_run'+run\n",
    "f = open(summary_name+'.txt', 'w')\n",
    "f.write('Run summary: task 3, subject number '+subject_num+', algorithm type: KNN, run '+run+'\\n')\n",
    "f.write('fine tuning sessions: ')\n",
    "for s_ind, s in enumerate(ft_train_sessions):\n",
    "  if s_ind != len(ft_train_sessions) - 1:\n",
    "    f.write(s+', ')\n",
    "  else:\n",
    "    f.write(s+'\\n')\n",
    "f.write('fine tuning positions: ')\n",
    "for p_ind, p in enumerate(ft_train_positions):\n",
    "  if p_ind != len(ft_train_positions) - 1:\n",
    "    f.write(p+', ')\n",
    "  else:\n",
    "    f.write(p+'\\n')   \n",
    "f.write('k: '+str(k_KNN)+'\\n')\n",
    "f.write('Single frame accuracy: '+str(single_acc)+'\\n')\n",
    "f.write('Accuracy after majority voting: '+str(accuracy)+'\\n')\n",
    "f.close()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1654349023390,
     "user_tz": -180,
     "elapsed": 1462,
     "user": {
      "displayName": "Adi Ben-Ari",
      "userId": "15772198121198769105"
     }
    },
    "outputId": "c274c594-77a8-4d21-fa7e-0a15c938f113",
    "id": "rPcYXHctCzJ0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SVM"
   ],
   "metadata": {
    "id": "TbxJAUTbCzJ1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Task 1** - SVM"
   ],
   "metadata": {
    "id": "0kRYykf1CzJ2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "run = '1'"
   ],
   "metadata": {
    "id": "-U0-CIB5CzJ2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "c = 100\n",
    "\n",
    "# Read the files:\n",
    "train_data = None\n",
    "train_labels = None\n",
    "test_data = None\n",
    "test_labels = None\n",
    "name_start_old = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "name_start_val_old = 'segmented_files/subject_'+subject_num+'/sf_'+subject_num\n",
    "name_start = 'segmented_files/subject_'#+subject_num+'/sf_'+subject_num\n",
    "name_start_val = 'segmented_files/subject_'#+subject_num+'/sf_'+subject_num\n",
    "#for filename in [(name_start+'_pos'+position+'_s'+session+'_train_data.pkl', name_start+'_pos'+position+'_s'+session+'_train_labels.pkl', None, None) for position, session in pos_session_list] + [(name_start+'_pos'+position+'_s'+session+'_test_data.pkl', name_start+'_pos'+position+'_s'+session+'_test_labels.pkl', name_start_val+'_pos'+position+'_s'+session+'_train_data.pkl', name_start_val+'_pos'+position+'_s'+session+'_train_labels.pkl') for position, session in ft_pos_session_list]:\n",
    "for filename in [(name_start+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'+session+'_train_data.pkl', name_start+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'+session+'_train_labels.pkl', None, None) for subject_num, position, session in sub_pos_session_list] + [(name_start_old+'_pos'+position+'_s'+session+'_test_data.pkl', name_start_old+'_pos'+position+'_s'+session+'_test_labels.pkl', name_start_val_old+'_pos'+position+'_s'+session+'_train_data.pkl', name_start_val_old+'_pos'+position+'_s'+session+'_train_labels.pkl') for position, session in ft_pos_session_list]:\n",
    "#for filename in [(name_start+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'+session+'_train_data.pkl', name_start+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'+session+'_train_labels.pkl', name_start_val+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'+session+'_val_data.pkl', name_start_val+subject_num+'/sf_'+subject_num+'_pos'+position+'_s'+session+'_val_labels.pkl') for subject_num, position, session in sub_pos_session_list]:\n",
    "    if filename[0] is not None:\n",
    "        data = pd.read_pickle(filename[0])\n",
    "        data = data.to_numpy()\n",
    "    else:\n",
    "        data = None\n",
    "    print('train data read!')\n",
    "    if filename[1] is not None:\n",
    "        labels = pd.read_pickle(filename[1])\n",
    "        labels = labels.to_numpy()\n",
    "    else:\n",
    "        labels = None\n",
    "    print('train labels read!')\n",
    "    if filename[2] is not None:\n",
    "        test_data_part = pd.read_pickle(filename[2])\n",
    "        test_data_part = test_data_part.to_numpy()\n",
    "    else:\n",
    "        test_data_part = None\n",
    "    print('test data read!')\n",
    "    if filename[3] is not None:\n",
    "        test_labels_part = pd.read_pickle(filename[3])\n",
    "        test_labels_part = test_labels_part.to_numpy()\n",
    "    else:\n",
    "        test_labels_part = None\n",
    "    print('test labels read!')\n",
    "\n",
    "    if train_data is not None and data is not None:\n",
    "        train_data = np.concatenate((train_data, data))\n",
    "        train_labels = np.concatenate((train_labels, labels))\n",
    "    if test_data is not None and test_data_part is not None:\n",
    "        test_data = np.concatenate((test_data, test_data_part))\n",
    "        test_labels = np.concatenate((test_labels, test_labels_part))\n",
    "    if train_data is None or test_data is None:\n",
    "        if data is not None and train_data is None:\n",
    "            train_data = copy.deepcopy(data)\n",
    "            train_labels = copy.deepcopy(labels)\n",
    "        if test_data_part is not None and test_data is None:\n",
    "            test_data = copy.deepcopy(test_data_part)\n",
    "            test_labels = copy.deepcopy(test_labels_part)\n",
    "\n",
    "#svm_classifier = pickle.load(open('svm_model.sav', 'rb'))\n",
    "svm_classifier = svm.SVC(C=c)\n",
    "svm_classifier.fit(train_data, train_labels)\n",
    "predictions = svm_classifier.predict(test_data).astype('int')\n",
    "single_acc = np.mean(predictions==test_labels.T)\n",
    "print('SVM single frame accuracy:')\n",
    "print(single_acc)\n",
    "SVM_model_name = 'weights/SVM/SVM_task3_'+subject_num+'_pos'+ft_train_positions[0]+'_run'+run\n",
    "pickle.dump(svm_classifier, open(SVM_model_name+'.sav', 'wb'))\n",
    "\n",
    "\n",
    "# majority voting:\n",
    "predictions_majority = np.zeros(60*len(ft_pos_session_list))\n",
    "prev = 0\n",
    "for i in range(60*len(ft_pos_session_list)):\n",
    "    relevant_pred = predictions[prev:prev+min_window]\n",
    "    predictions_majority[i] = np.argmax(np.bincount(relevant_pred))\n",
    "    prev = prev+int(min_window)\n",
    "print('predictions after majority voting:')\n",
    "print(predictions_majority)\n",
    "predictions_majority = predictions_majority.astype(int)\n",
    "real_labels = np.array([0,0,0,0,0,0,1,1,1,1,1,1,2,2,2,2,2,2,3,3,3,3,3,3,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,6,6,6,7,7,7,7,7,7,8,8,8,8,8,8,9,9,9,9,9,9]*len(ft_pos_session_list))\n",
    "accuracy = np.mean(predictions_majority==real_labels)\n",
    "print('Real labels:')\n",
    "print(real_labels)\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "print('Test accuracy after majority voting:')\n",
    "print(accuracy)\n",
    "print('Confusion matrix:')\n",
    "plt.imshow(confusion_matrix(real_labels, predictions_majority))\n",
    "plt.colorbar()\n",
    "#plt.show()\n",
    "summary_name = 'summaries/subject_'+subject_num+'/task3/summary_task3_'+subject_num+'_pos'+position+'_type_SVM_run'+run\n",
    "f = open(summary_name+'.txt', 'w')\n",
    "f.write('Run summary: task 3, subject number '+subject_num+', algorithm type: SVM, run '+run+'\\n')\n",
    "f.write('fine tuning sessions: ')\n",
    "for s_ind, s in enumerate(ft_train_sessions):\n",
    "  if s_ind != len(ft_train_sessions) - 1:\n",
    "    f.write(s+', ')\n",
    "  else:\n",
    "    f.write(s+'\\n')\n",
    "f.write('fine tuning positions: ')\n",
    "for p_ind, p in enumerate(ft_train_positions):\n",
    "  if p_ind != len(ft_train_positions) - 1:\n",
    "    f.write(p+', ')\n",
    "  else:\n",
    "    f.write(p+'\\n')   \n",
    "f.write('C: '+str(c)+'\\n')\n",
    "f.write('model name: '+SVM_model_name+'.sav'+'\\n')\n",
    "f.write('single frame accuracy: '+str(single_acc)+'\\n')\n",
    "f.write('accuracy after majority voting: '+str(accuracy)+'\\n')\n",
    "f.close()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1654349039818,
     "user_tz": -180,
     "elapsed": 3276,
     "user": {
      "displayName": "Adi Ben-Ari",
      "userId": "15772198121198769105"
     }
    },
    "outputId": "fc60d3b1-5a36-464a-f29d-03363cbca30d",
    "id": "OQmpxyNaCzJ2"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "dG534j6kDpiV",
    "cEQ5aeW7DkDw",
    "SswWRqs-ewk0",
    "Nu_EVkCODudF",
    "96ogrpPAilx_",
    "ulvFa7TuCzJU",
    "Ii2CmKR3CzJg"
   ],
   "name": "codes_Adi_Liron.ipynb",
   "provenance": [
    {
     "file_id": "1vbSQl-lGKwARfc8ckNNKO7pDmVryDE0H",
     "timestamp": 1654016188966
    },
    {
     "file_id": "1q4Q1SK9d0YYLY-sAvtlP4FTECUvuWfGd",
     "timestamp": 1653726664349
    },
    {
     "file_id": "1T0meyQgM22u5xI5WxUzqsc0_OUWYpO0x",
     "timestamp": 1653484143854
    },
    {
     "file_id": "1fRYekgRRjY1G0FaqL4RhVRVpYKLTe8C9",
     "timestamp": 1650885321375
    },
    {
     "file_id": "1TAhGo0ZXqrKON_Cs79WPoEuZ_Nzo6BiM",
     "timestamp": 1650443287313
    },
    {
     "file_id": "1W4D1_2Emw6W_rudhpFyYgGhzi49tIG8k",
     "timestamp": 1649490032030
    },
    {
     "file_id": "1fJzKjeotaq_I8WSp8-YHk0DDRauP5llG",
     "timestamp": 1649421400470
    },
    {
     "file_id": "17G_ATDvM8c2I4A4HbdVmMkfSCQofD6u_",
     "timestamp": 1648973789243
    },
    {
     "file_id": "1RT35Dj5Q3QCa9bpP2kSG9PeL0Jokxnr0",
     "timestamp": 1648913930447
    },
    {
     "file_id": "1Blkio3zT327VHjDKUi8ws4wGwXjWgrLC",
     "timestamp": 1645114465510
    },
    {
     "file_id": "1w9QPhSoYlgQTdZIlxnLAWL_zVZWjHlO1",
     "timestamp": 1644482128188
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
